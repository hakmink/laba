{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering(Manual)\n",
    "### Import, Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package load\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "dicpath = os.getcwd()\n",
    "\n",
    "\n",
    "def len_consec_zeros(a):\n",
    "    a = np.array(list(a))    # convert elements to `str`\n",
    "    rr = np.argwhere(a == '0').ravel()  # find out positions of `0`\n",
    "    if not rr.size:  # if there are no zeros, return 0\n",
    "        return 0\n",
    "\n",
    "    full = np.arange(rr[0], rr[-1]+1)  # get the range of spread of 0s\n",
    "\n",
    "    # get the indices where `0` was flipped to something else\n",
    "    diff = np.setdiff1d(full, rr)\n",
    "    if not diff.size:     # if there are no bit flips, return the \n",
    "        return len(full)  # size of the full range\n",
    "\n",
    "    # break the array into pieces wherever there's a bit flip\n",
    "    # and the result is the size of the largest chunk\n",
    "    pos, difs = full[0], []\n",
    "    for el in diff:\n",
    "        difs.append(el - pos)\n",
    "        pos = el + 1\n",
    "\n",
    "    difs.append(full[-1]+1 - pos)\n",
    "\n",
    "    # return size of the largest chunk\n",
    "    res = max(difs) if max(difs) != 1 else 0\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "#data load\n",
    "train_activity = pd.read_csv(dicpath+'/train/train_activity.csv',encoding='EUC-kr')\n",
    "train_label = pd.read_csv(dicpath+'/train/train_label.csv',encoding='EUC-kr')\n",
    "train_guild = pd.read_csv(dicpath+'/train/train_guild.csv',encoding='EUC-kr')\n",
    "train_party = pd.read_csv(dicpath+'/train/train_party.csv',encoding='EUC-kr')\n",
    "train_payment = pd.read_csv(dicpath+'/train/train_payment.csv',encoding='EUC-kr')\n",
    "train_trade = pd.read_csv(dicpath+'/train/train_trade.csv',encoding='EUC-kr')\n",
    "\n",
    "test_activity = pd.read_csv(dicpath+'/test/test_activity.csv',encoding='EUC-kr')\n",
    "test_guild = pd.read_csv(dicpath+'/test/test_guild.csv',encoding='EUC-kr')\n",
    "test_party = pd.read_csv(dicpath+'/test/test_party.csv',encoding='EUC-kr')\n",
    "test_payment = pd.read_csv(dicpath+'/test/test_payment.csv',encoding='EUC-kr')\n",
    "test_trade = pd.read_csv(dicpath+'/test/test_trade.csv',encoding='EUC-kr')\n",
    "test_label = pd.DataFrame(test_payment[\"acc_id\"].unique())\n",
    "test_label.columns = [\"acc_id\"]\n",
    "\n",
    "train_label = train_label.append(test_label)\n",
    "train_activity = train_activity.append(test_activity)\n",
    "train_guild = train_guild.append(test_guild)\n",
    "train_party = train_party.append(test_party)\n",
    "train_payment = train_payment.append(test_party)\n",
    "train_trade = train_trade.append(test_trade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering(Manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#생성된 파생변수를 담을 데이터 프레임 생성\n",
    "act_ft_eng=pd.DataFrame({\"acc_id\":train_activity[\"acc_id\"],\"wk\":train_activity[\"wk\"]})\n",
    "\n",
    "#play_time related\n",
    "act_ft_eng[\"npc_exp_div_play_time\"] = np.exp(train_activity[\"npc_exp\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"npc_hongmun_div_play_time\"] = np.exp(train_activity[\"npc_hongmun\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"quest_exp_div_play_time\"] = np.exp(train_activity[\"quest_exp\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"quest_hongmun_div_play_time\"] = np.exp(train_activity[\"quest_hongmun\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"item_hongmun_div_play_time\"] = np.exp(train_activity[\"item_hongmun\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"game_combat_time_div_play_time\"] = np.exp(train_activity[\"game_combat_time\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"get_money_div_play_time\"] = np.exp(train_activity[\"get_money\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"normal_chat_div_play_time\"] = np.exp(train_activity[\"normal_chat\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"whisper_chat_div_play_time\"] = np.exp(train_activity[\"whisper_chat\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"district_chat_div_play_time\"] = np.exp(train_activity[\"district_chat\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"party_chat_div_play_time\"] = np.exp(train_activity[\"party_chat\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"guild_chat_div_play_time\"] = np.exp(train_activity[\"guild_chat\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"faction_chat_div_play_time\"] = np.exp(train_activity[\"faction_chat\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"gathering_cnt_div_play_time\"] = np.exp(train_activity[\"gathering_cnt\"])/np.exp(train_activity[\"play_time\"])\n",
    "act_ft_eng[\"making_cnt_div_play_time\"] = np.exp(train_activity[\"making_cnt\"])/np.exp(train_activity[\"play_time\"])\n",
    "\n",
    "#cnt_dt related\n",
    "act_ft_eng[\"play_time_div_cnt_dt\"] = np.exp(train_activity[\"play_time\"])/np.exp(train_activity[\"cnt_dt\"])\n",
    "\n",
    "#game_combat_time related\n",
    "act_ft_eng[\"npc_exp_div_game_combat_time\"] = np.exp(train_activity[\"npc_exp\"])/np.exp(train_activity[\"game_combat_time\"])\n",
    "act_ft_eng[\"npc_hongmun_div_game_combat_time\"] = np.exp(train_activity[\"npc_hongmun\"])/np.exp(train_activity[\"game_combat_time\"])\n",
    "act_ft_eng[\"quest_exp_div_game_combat_time\"] = np.exp(train_activity[\"quest_exp\"])/np.exp(train_activity[\"game_combat_time\"])\n",
    "act_ft_eng[\"quest_hongmun_div_game_combat_time\"] = np.exp(train_activity[\"quest_hongmun\"])/np.exp(train_activity[\"game_combat_time\"])\n",
    "act_ft_eng[\"item_hongmun_div_game_combat_time\"] = np.exp(train_activity[\"item_hongmun\"])/np.exp(train_activity[\"game_combat_time\"])\n",
    "act_ft_eng[\"get_money_div_game_combat_time\"] = np.exp(train_activity[\"get_money\"])/np.exp(train_activity[\"game_combat_time\"])\n",
    "\n",
    "#duel related\n",
    "act_ft_eng[\"duel_win_div_duel_cnt\"] = np.exp(train_activity[\"duel_win\"])/np.exp(train_activity[\"duel_cnt\"])\n",
    "act_ft_eng[\"partyb_win_div_partyb_cnt\"] = np.exp(train_activity[\"partybattle_win\"])/np.exp(train_activity[\"partybattle_cnt\"])\n",
    "\n",
    "#raid related\n",
    "act_ft_eng[\"cnt_enter_inzone_solo_div_cnt_enter_raid\"] = np.exp(train_activity[\"cnt_enter_inzone_solo\"])/np.exp(train_activity[\"cnt_enter_raid\"])\n",
    "act_ft_eng[\"cnt_clear_raid_div_cnt_enter_raid\"] = np.exp(train_activity[\"cnt_clear_raid\"])/np.exp(train_activity[\"cnt_enter_raid\"])\n",
    "\n",
    "#train_activity 데이터에 새로 생성한 파생변수 붙여넣기\n",
    "train_activity = train_activity.merge(act_ft_eng,how=\"left\")\n",
    "\n",
    "#수동으로 aggregate + 가중 평균 + 시차 변수 생성 with count\n",
    "tmp_cnt_agg = train_activity[train_activity.wk == 8][['acc_id','cnt_dt']]\\\n",
    ".merge(train_activity[train_activity.wk == 7][['acc_id','cnt_dt']],how='left',on=\"acc_id\")\\\n",
    ".merge(train_activity[train_activity.wk == 6][['acc_id','cnt_dt']],how='left',on=\"acc_id\")\\\n",
    ".merge(train_activity[train_activity.wk == 5][['acc_id','cnt_dt']],how='left',on=\"acc_id\")\\\n",
    ".merge(train_activity[train_activity.wk == 4][['acc_id','cnt_dt']],how='left',on=\"acc_id\")\\\n",
    ".merge(train_activity[train_activity.wk == 3][['acc_id','cnt_dt']],how='left',on=\"acc_id\")\\\n",
    ".merge(train_activity[train_activity.wk == 2][['acc_id','cnt_dt']],how='left',on=\"acc_id\")\\\n",
    ".merge(train_activity[train_activity.wk == 1][['acc_id','cnt_dt']],how='left',on=\"acc_id\")\n",
    "tmp_cnt_agg.columns = [\"acc_id\",'wk8','wk7','wk6','wk5','wk4','wk3','wk2','wk1']\n",
    "tmp_cnt_agg = tmp_cnt_agg.fillna(0)\n",
    "for column in tmp_cnt_agg:\n",
    "    if column != 'acc_id':\n",
    "        tmp_cnt_agg[column] = tmp_cnt_agg[column].astype(int)\n",
    "tmp_cnt_agg[\"mean_wk\"] = tmp_cnt_agg.drop(['acc_id'],axis=1).mean(axis=1)\n",
    "tmp_cnt_agg[\"std_wk\"] = tmp_cnt_agg.drop(['acc_id'],axis=1).std(axis=1)\n",
    "tmp_cnt_agg[\"wgt_mean_wk\"] = (tmp_cnt_agg[\"wk8\"]*8+tmp_cnt_agg[\"wk7\"]*7+tmp_cnt_agg[\"wk6\"]*6+tmp_cnt_agg[\"wk5\"]*5+tmp_cnt_agg[\"wk4\"]*4+tmp_cnt_agg[\"wk3\"]*3+tmp_cnt_agg[\"wk2\"]*2+tmp_cnt_agg[\"wk1\"])/36\n",
    "tmp_cnt_agg[\"cnt_lag_78\"] = tmp_cnt_agg[\"wk8\"]-tmp_cnt_agg[\"wk7\"]\n",
    "tmp_cnt_agg[\"cnt_lag_68\"] = tmp_cnt_agg[\"wk8\"]-tmp_cnt_agg[\"wk6\"]\n",
    "\n",
    "#수동으로 aggregate + 가중 평균 + 시차 변수 생성 with time\n",
    "tmp_time_agg = train_activity[train_activity.wk == 8][['acc_id','play_time']]\\\n",
    ".merge(train_activity[train_activity.wk == 7][['acc_id','play_time']],how='left',on=\"acc_id\")\\\n",
    ".merge(train_activity[train_activity.wk == 6][['acc_id','play_time']],how='left',on=\"acc_id\")\\\n",
    ".merge(train_activity[train_activity.wk == 5][['acc_id','play_time']],how='left',on=\"acc_id\")\\\n",
    ".merge(train_activity[train_activity.wk == 4][['acc_id','play_time']],how='left',on=\"acc_id\")\\\n",
    ".merge(train_activity[train_activity.wk == 3][['acc_id','play_time']],how='left',on=\"acc_id\")\\\n",
    ".merge(train_activity[train_activity.wk == 2][['acc_id','play_time']],how='left',on=\"acc_id\")\\\n",
    ".merge(train_activity[train_activity.wk == 1][['acc_id','play_time']],how='left',on=\"acc_id\")\n",
    "tmp_time_agg.columns = [\"acc_id\",'play_time_wk8','play_time_wk7','play_time_wk6','play_time_wk5','play_time_wk4','play_time_wk3','play_time_wk2','play_time_wk1']\n",
    "tmp_time_agg = tmp_time_agg.fillna(-0.6617)\n",
    "\n",
    "tmp_time_agg[\"mean_wk_pt\"] = tmp_time_agg.drop(['acc_id'],axis=1).mean(axis=1)\n",
    "tmp_time_agg[\"std_wk_pt\"] = tmp_time_agg.drop(['acc_id'],axis=1).std(axis=1)\n",
    "tmp_time_agg[\"wgt_mean_wk_pt\"] = (tmp_time_agg[\"play_time_wk8\"]*8+tmp_time_agg[\"play_time_wk7\"]*7+tmp_time_agg[\"play_time_wk6\"]*6+tmp_time_agg[\"play_time_wk5\"]*5+tmp_time_agg[\"play_time_wk4\"]*4+tmp_time_agg[\"play_time_wk3\"]*3+tmp_time_agg[\"play_time_wk2\"]*2+tmp_time_agg[\"play_time_wk1\"])/36\n",
    "tmp_time_agg[\"time_lag_78\"] = tmp_time_agg[\"play_time_wk8\"]-tmp_time_agg[\"play_time_wk7\"]\n",
    "tmp_time_agg[\"time_lag_68\"] = tmp_time_agg[\"play_time_wk8\"]-tmp_time_agg[\"play_time_wk6\"]\n",
    "\n",
    "\n",
    "#수동으로 aggregate + 가중 평균 + 시차 변수 생성 with time/count\n",
    "tmp_div_time_agg = pd.merge(tmp_cnt_agg,tmp_time_agg,how='left',on='acc_id').drop(['mean_wk','std_wk','wgt_mean_wk','mean_wk_pt','std_wk_pt','wgt_mean_wk_pt'],axis=1)\n",
    "tmp_dv_time_agg = pd.DataFrame(tmp_div_time_agg['acc_id'])\n",
    "tmp_dv_time_agg[\"dv_pt_cnt_wk8\"] = np.exp(tmp_time_agg[\"play_time_wk8\"])/tmp_cnt_agg[\"wk8\"]\n",
    "tmp_dv_time_agg[\"dv_pt_cnt_wk7\"] = np.exp(tmp_time_agg[\"play_time_wk7\"])/tmp_cnt_agg[\"wk7\"]\n",
    "tmp_dv_time_agg[\"dv_pt_cnt_wk6\"] = np.exp(tmp_time_agg[\"play_time_wk6\"])/tmp_cnt_agg[\"wk6\"]\n",
    "tmp_dv_time_agg[\"dv_pt_cnt_wk5\"] = np.exp(tmp_time_agg[\"play_time_wk5\"])/tmp_cnt_agg[\"wk5\"]\n",
    "tmp_dv_time_agg[\"dv_pt_cnt_wk4\"] = np.exp(tmp_time_agg[\"play_time_wk4\"])/tmp_cnt_agg[\"wk4\"]\n",
    "tmp_dv_time_agg[\"dv_pt_cnt_wk3\"] = np.exp(tmp_time_agg[\"play_time_wk3\"])/tmp_cnt_agg[\"wk3\"]\n",
    "tmp_dv_time_agg[\"dv_pt_cnt_wk2\"] = np.exp(tmp_time_agg[\"play_time_wk2\"])/tmp_cnt_agg[\"wk2\"]\n",
    "tmp_dv_time_agg[\"dv_pt_cnt_wk1\"] = np.exp(tmp_time_agg[\"play_time_wk1\"])/tmp_cnt_agg[\"wk1\"]\n",
    "tmp_dv_time_agg = tmp_dv_time_agg.replace([np.inf],0)\n",
    "\n",
    "tmp_dv_time_agg['mean_dv_pt_cnt'] = tmp_dv_time_agg.drop(['acc_id'],axis=1).mean(axis=1)\n",
    "tmp_dv_time_agg['std_dv_pt_cnt'] = tmp_dv_time_agg.drop(['acc_id'],axis=1).std(axis=1)\n",
    "tmp_dv_time_agg['wgt_dv_pt_cnt'] = (tmp_dv_time_agg[\"dv_pt_cnt_wk8\"]*8+tmp_dv_time_agg[\"dv_pt_cnt_wk7\"]*7+tmp_dv_time_agg[\"dv_pt_cnt_wk6\"]*6+tmp_dv_time_agg[\"dv_pt_cnt_wk5\"]*5+tmp_dv_time_agg[\"dv_pt_cnt_wk4\"]*4+tmp_dv_time_agg[\"dv_pt_cnt_wk3\"]*3+tmp_dv_time_agg[\"dv_pt_cnt_wk2\"]*2+tmp_dv_time_agg[\"dv_pt_cnt_wk1\"])/36\n",
    "tmp_dv_time_agg['dv_tc_lag_78'] = tmp_dv_time_agg[\"dv_pt_cnt_wk8\"]-tmp_dv_time_agg[\"dv_pt_cnt_wk7\"]\n",
    "tmp_dv_time_agg['dv_tc_lag_68'] = tmp_dv_time_agg[\"dv_pt_cnt_wk8\"]-tmp_dv_time_agg[\"dv_pt_cnt_wk6\"]\n",
    "\n",
    "\n",
    "# feature_activity\n",
    "act_ft_eng_01 = act_ft_eng[act_ft_eng.wk == 8].drop('wk',axis=1)\n",
    "\n",
    "for i in range(7):\n",
    "    temp = act_ft_eng[act_ft_eng.wk == i].drop(['acc_id','wk'],axis=1)\n",
    "    temp.columns = temp.columns + str(i+1)\n",
    "    temp[\"acc_id\"] = act_ft_eng[act_ft_eng.wk == i].drop('wk',axis=1)['acc_id']\n",
    "    \n",
    "    act_ft_eng_01 = pd.merge(act_ft_eng_01,temp,how='left',on='acc_id')\n",
    "    \n",
    "#마지막 연속 미접속일 지속 날짜\n",
    "tmp_train_01 = tmp_cnt_agg[[\"wk1\",'wk2','wk3','wk4','wk5','wk6','wk7','wk8']]\n",
    "\n",
    "tmp_train_01['concated'] = tmp_train_01.astype(str).apply(lambda x: ''.join(x), axis=1)\n",
    "tmp_train_01['consecutive_zeros'] = tmp_train_01.concated.apply(lambda x: len_consec_zeros(x))\n",
    "tmp_cnt_agg[\"consecutive_zero\"] = tmp_train_01['consecutive_zeros'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = train_label.merge(tmp_cnt_agg,how='left',on='acc_id')\\\n",
    ".merge(tmp_time_agg,how='left',on='acc_id')\\\n",
    ".merge(tmp_dv_time_agg,how='left',on='acc_id')\\\n",
    ".merge(act_ft_eng_01,how='left',on='acc_id')\\\n",
    ".fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 250) (40000, 250)\n"
     ]
    }
   ],
   "source": [
    "test_all_n = train_all[train_all.label == 0]\n",
    "train_all_n = train_all[train_all.label != 0]\n",
    "print(train_all_n.shape,test_all_n.shape)\n",
    "\n",
    "test_all_n=test_all_n.drop('label',axis=1)\n",
    "train_all_n.to_csv(\"fin_train_02.csv\")\n",
    "test_all_n.to_csv(\"fin_test_02.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a408108/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "train_all_n[\"label\"] = le.fit_transform(train_all_n[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.702753\n",
      "[100]\tvalid_0's multi_logloss: 0.666265\n",
      "[150]\tvalid_0's multi_logloss: 0.653588\n",
      "[200]\tvalid_0's multi_logloss: 0.643878\n",
      "[250]\tvalid_0's multi_logloss: 0.636598\n",
      "[300]\tvalid_0's multi_logloss: 0.630786\n",
      "[350]\tvalid_0's multi_logloss: 0.62593\n",
      "[400]\tvalid_0's multi_logloss: 0.622167\n",
      "[450]\tvalid_0's multi_logloss: 0.619361\n",
      "[500]\tvalid_0's multi_logloss: 0.616986\n",
      "[550]\tvalid_0's multi_logloss: 0.614708\n",
      "[600]\tvalid_0's multi_logloss: 0.612878\n",
      "[650]\tvalid_0's multi_logloss: 0.611564\n",
      "[700]\tvalid_0's multi_logloss: 0.609974\n",
      "[750]\tvalid_0's multi_logloss: 0.608787\n",
      "[800]\tvalid_0's multi_logloss: 0.607869\n",
      "[850]\tvalid_0's multi_logloss: 0.60693\n",
      "[900]\tvalid_0's multi_logloss: 0.606132\n",
      "[950]\tvalid_0's multi_logloss: 0.60533\n",
      "[1000]\tvalid_0's multi_logloss: 0.604629\n",
      "[1050]\tvalid_0's multi_logloss: 0.604406\n",
      "[1100]\tvalid_0's multi_logloss: 0.604013\n",
      "[1150]\tvalid_0's multi_logloss: 0.603751\n",
      "[1200]\tvalid_0's multi_logloss: 0.603628\n",
      "[1250]\tvalid_0's multi_logloss: 0.60371\n",
      "[1300]\tvalid_0's multi_logloss: 0.603689\n",
      "[1350]\tvalid_0's multi_logloss: 0.603639\n",
      "[1400]\tvalid_0's multi_logloss: 0.603799\n",
      "Early stopping, best iteration is:\n",
      "[1207]\tvalid_0's multi_logloss: 0.603575\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.694103\n",
      "[100]\tvalid_0's multi_logloss: 0.65915\n",
      "[150]\tvalid_0's multi_logloss: 0.646584\n",
      "[200]\tvalid_0's multi_logloss: 0.637834\n",
      "[250]\tvalid_0's multi_logloss: 0.631502\n",
      "[300]\tvalid_0's multi_logloss: 0.626918\n",
      "[350]\tvalid_0's multi_logloss: 0.623121\n",
      "[400]\tvalid_0's multi_logloss: 0.620144\n",
      "[450]\tvalid_0's multi_logloss: 0.617759\n",
      "[500]\tvalid_0's multi_logloss: 0.615699\n",
      "[550]\tvalid_0's multi_logloss: 0.613799\n",
      "[600]\tvalid_0's multi_logloss: 0.611988\n",
      "[650]\tvalid_0's multi_logloss: 0.61046\n",
      "[700]\tvalid_0's multi_logloss: 0.609123\n",
      "[750]\tvalid_0's multi_logloss: 0.608011\n",
      "[800]\tvalid_0's multi_logloss: 0.607122\n",
      "[850]\tvalid_0's multi_logloss: 0.606481\n",
      "[900]\tvalid_0's multi_logloss: 0.605986\n",
      "[950]\tvalid_0's multi_logloss: 0.605235\n",
      "[1000]\tvalid_0's multi_logloss: 0.604581\n",
      "[1050]\tvalid_0's multi_logloss: 0.604336\n",
      "[1100]\tvalid_0's multi_logloss: 0.604125\n",
      "[1150]\tvalid_0's multi_logloss: 0.604189\n",
      "[1200]\tvalid_0's multi_logloss: 0.603959\n",
      "[1250]\tvalid_0's multi_logloss: 0.60397\n",
      "[1300]\tvalid_0's multi_logloss: 0.604082\n",
      "[1350]\tvalid_0's multi_logloss: 0.604268\n",
      "[1400]\tvalid_0's multi_logloss: 0.604382\n",
      "Early stopping, best iteration is:\n",
      "[1208]\tvalid_0's multi_logloss: 0.603853\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.70038\n",
      "[100]\tvalid_0's multi_logloss: 0.664151\n",
      "[150]\tvalid_0's multi_logloss: 0.652264\n",
      "[200]\tvalid_0's multi_logloss: 0.644396\n",
      "[250]\tvalid_0's multi_logloss: 0.638077\n",
      "[300]\tvalid_0's multi_logloss: 0.63317\n",
      "[350]\tvalid_0's multi_logloss: 0.629486\n",
      "[400]\tvalid_0's multi_logloss: 0.625992\n",
      "[450]\tvalid_0's multi_logloss: 0.62304\n",
      "[500]\tvalid_0's multi_logloss: 0.620592\n",
      "[550]\tvalid_0's multi_logloss: 0.618477\n",
      "[600]\tvalid_0's multi_logloss: 0.616527\n",
      "[650]\tvalid_0's multi_logloss: 0.615009\n",
      "[700]\tvalid_0's multi_logloss: 0.613712\n",
      "[750]\tvalid_0's multi_logloss: 0.612252\n",
      "[800]\tvalid_0's multi_logloss: 0.611202\n",
      "[850]\tvalid_0's multi_logloss: 0.610267\n",
      "[900]\tvalid_0's multi_logloss: 0.609428\n",
      "[950]\tvalid_0's multi_logloss: 0.608991\n",
      "[1000]\tvalid_0's multi_logloss: 0.608791\n",
      "[1050]\tvalid_0's multi_logloss: 0.608133\n",
      "[1100]\tvalid_0's multi_logloss: 0.607659\n",
      "[1150]\tvalid_0's multi_logloss: 0.607319\n",
      "[1200]\tvalid_0's multi_logloss: 0.607166\n",
      "[1250]\tvalid_0's multi_logloss: 0.607105\n",
      "[1300]\tvalid_0's multi_logloss: 0.607211\n",
      "[1350]\tvalid_0's multi_logloss: 0.607312\n",
      "[1400]\tvalid_0's multi_logloss: 0.607317\n",
      "Early stopping, best iteration is:\n",
      "[1213]\tvalid_0's multi_logloss: 0.606996\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.700116\n",
      "[100]\tvalid_0's multi_logloss: 0.664311\n",
      "[150]\tvalid_0's multi_logloss: 0.651871\n",
      "[200]\tvalid_0's multi_logloss: 0.642347\n",
      "[250]\tvalid_0's multi_logloss: 0.635921\n",
      "[300]\tvalid_0's multi_logloss: 0.630232\n",
      "[350]\tvalid_0's multi_logloss: 0.625996\n",
      "[400]\tvalid_0's multi_logloss: 0.622182\n",
      "[450]\tvalid_0's multi_logloss: 0.619244\n",
      "[500]\tvalid_0's multi_logloss: 0.616576\n",
      "[550]\tvalid_0's multi_logloss: 0.614633\n",
      "[600]\tvalid_0's multi_logloss: 0.6125\n",
      "[650]\tvalid_0's multi_logloss: 0.61088\n",
      "[700]\tvalid_0's multi_logloss: 0.609495\n",
      "[750]\tvalid_0's multi_logloss: 0.608511\n",
      "[800]\tvalid_0's multi_logloss: 0.60781\n",
      "[850]\tvalid_0's multi_logloss: 0.607202\n",
      "[900]\tvalid_0's multi_logloss: 0.606306\n",
      "[950]\tvalid_0's multi_logloss: 0.605522\n",
      "[1000]\tvalid_0's multi_logloss: 0.605132\n",
      "[1050]\tvalid_0's multi_logloss: 0.604552\n",
      "[1100]\tvalid_0's multi_logloss: 0.604139\n",
      "[1150]\tvalid_0's multi_logloss: 0.604104\n",
      "[1200]\tvalid_0's multi_logloss: 0.603991\n",
      "[1250]\tvalid_0's multi_logloss: 0.603889\n",
      "[1300]\tvalid_0's multi_logloss: 0.603974\n",
      "[1350]\tvalid_0's multi_logloss: 0.603797\n",
      "[1400]\tvalid_0's multi_logloss: 0.603741\n",
      "[1450]\tvalid_0's multi_logloss: 0.60397\n",
      "[1500]\tvalid_0's multi_logloss: 0.604105\n",
      "[1550]\tvalid_0's multi_logloss: 0.604327\n",
      "[1600]\tvalid_0's multi_logloss: 0.604457\n",
      "Early stopping, best iteration is:\n",
      "[1401]\tvalid_0's multi_logloss: 0.603733\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.703776\n",
      "[100]\tvalid_0's multi_logloss: 0.668179\n",
      "[150]\tvalid_0's multi_logloss: 0.655387\n",
      "[200]\tvalid_0's multi_logloss: 0.647466\n",
      "[250]\tvalid_0's multi_logloss: 0.640238\n",
      "[300]\tvalid_0's multi_logloss: 0.634759\n",
      "[350]\tvalid_0's multi_logloss: 0.629975\n",
      "[400]\tvalid_0's multi_logloss: 0.626102\n",
      "[450]\tvalid_0's multi_logloss: 0.62293\n",
      "[500]\tvalid_0's multi_logloss: 0.620472\n",
      "[550]\tvalid_0's multi_logloss: 0.618257\n",
      "[600]\tvalid_0's multi_logloss: 0.616629\n",
      "[650]\tvalid_0's multi_logloss: 0.615072\n",
      "[700]\tvalid_0's multi_logloss: 0.613578\n",
      "[750]\tvalid_0's multi_logloss: 0.612395\n",
      "[800]\tvalid_0's multi_logloss: 0.611212\n",
      "[850]\tvalid_0's multi_logloss: 0.610471\n",
      "[900]\tvalid_0's multi_logloss: 0.609713\n",
      "[950]\tvalid_0's multi_logloss: 0.609395\n",
      "[1000]\tvalid_0's multi_logloss: 0.608922\n",
      "[1050]\tvalid_0's multi_logloss: 0.608565\n",
      "[1100]\tvalid_0's multi_logloss: 0.608139\n",
      "[1150]\tvalid_0's multi_logloss: 0.607762\n",
      "[1200]\tvalid_0's multi_logloss: 0.607637\n",
      "[1250]\tvalid_0's multi_logloss: 0.607332\n",
      "[1300]\tvalid_0's multi_logloss: 0.607256\n",
      "[1350]\tvalid_0's multi_logloss: 0.60727\n",
      "[1400]\tvalid_0's multi_logloss: 0.607292\n",
      "[1450]\tvalid_0's multi_logloss: 0.607208\n",
      "[1500]\tvalid_0's multi_logloss: 0.607321\n",
      "[1550]\tvalid_0's multi_logloss: 0.607539\n",
      "[1600]\tvalid_0's multi_logloss: 0.607954\n",
      "Early stopping, best iteration is:\n",
      "[1430]\tvalid_0's multi_logloss: 0.607153\n",
      "CPU times: user 46min 41s, sys: 5min 55s, total: 52min 36s\n",
      "Wall time: 13min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "# Create arrays and dataframes to store results\n",
    "feats = [f for f in train_all_n.columns if f not in ['label','acc_id']]\n",
    "folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "\n",
    "result_c = []\n",
    "\n",
    "\n",
    "num_trials = 1\n",
    "num_rounds = 3000\n",
    "early_stopping_rounds = 200\n",
    "params = {\n",
    "    'objective':['multiclass'],\n",
    "    'num_class':[4],\n",
    "    'class_weight':['balanced'],\n",
    "    'boosting':['gbdt'],\n",
    "    #'min_child_weight': list(np.arange(1,20,1)),\n",
    "    'colsample_bytree': [0.6],\n",
    "    #'max_depth': list(np.arange(3,15,1)),\n",
    "    #'subsample': list(np.arange(0.4,1,0.1))\n",
    "    'reg_alpha': [1],\n",
    "    'reg_lambda': [10]\n",
    "    #'learning_rate': [0.005,0.01,0.05,0.1],\n",
    "    #'num_leaves' :list(np.arange(20,55,3)),\n",
    "}\n",
    "\n",
    "for trial, param in zip(np.arange(num_trials),list(ParameterSampler(params, n_iter=num_trials))):\n",
    "    result_b = []\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_all_n[feats], train_all_n['label'])):\n",
    "        train_x, train_y = train_all_n[feats].iloc[train_idx], train_all_n['label'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_all_n[feats].iloc[valid_idx], train_all_n['label'].iloc[valid_idx] \n",
    "\n",
    "\n",
    "        lgb_train = lgb.Dataset(train_x, train_y)\n",
    "        lgb_eval = lgb.Dataset(valid_x, valid_y, reference=lgb_train)\n",
    "        \n",
    "        regressor = lgb.train(param,\n",
    "                              lgb_train,\n",
    "                              valid_sets = lgb_eval,\n",
    "                              num_boost_round = num_rounds,\n",
    "                              verbose_eval=50,\n",
    "                              early_stopping_rounds=early_stopping_rounds\n",
    "                             )\n",
    "        y_pred = regressor.predict(valid_x)\n",
    "\n",
    "\n",
    "        max_pred = []\n",
    "        for i in range(len(y_pred)):\n",
    "            max_pred.append(np.argmax(y_pred[i]))\n",
    "\n",
    "        comp_lab = pd.DataFrame({'pred':max_pred,'true':valid_y})\n",
    "\n",
    "        pr_rc = pd.DataFrame()\n",
    "        for i in range(len(comp_lab['pred'].value_counts())):\n",
    "            prec = len(comp_lab[(comp_lab[\"pred\"]==i) & (comp_lab[\"true\"]==i)])/len(comp_lab[\"pred\"][comp_lab[\"pred\"]==i])\n",
    "            recl = len(comp_lab[(comp_lab[\"pred\"]==i) & (comp_lab[\"true\"]==i)])/len(comp_lab[\"true\"][comp_lab[\"true\"]==i])\n",
    "\n",
    "            if i == 0:\n",
    "                pr_rc = pd.DataFrame({\"pred\":[prec],\"recl\":[recl]})\n",
    "            else:\n",
    "                pr_rc = pr_rc.append(pd.DataFrame({\"pred\":[prec],\"recl\":[recl]}))\n",
    "\n",
    "        f_score = list()\n",
    "        for i in range(len(pr_rc.index)):\n",
    "            f_score.append(1/(pr_rc[\"pred\"].tolist()[i]))\n",
    "            f_score.append(1/(pr_rc[\"recl\"].tolist()[i]))\n",
    "        ls_result_lgb = 8/sum(f_score)\n",
    "        \n",
    "        result_b.append(ls_result_lgb)\n",
    "        \n",
    "    result_c.append(result_b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용된 feature 개수 :  248\n",
      "5-fold CV f-score : [0.7191539760973668, 0.7157350381340298, 0.7182029981998694, 0.7223985117459301, 0.7233840290865586] average score : 0.7197749106527509\n",
      "predict value ratio :  5257 4190 5191 5362\n",
      "precision :  0.6431424766977364 , recall :  0.6713661636219221\n",
      "precision :  0.6699284009546539 , recall :  0.5730910575745202\n",
      "precision :  0.7927181660566365 , recall :  0.8220135836995606\n",
      "precision :  0.8453935098843715 , recall :  0.8958498023715415\n"
     ]
    }
   ],
   "source": [
    "###lgb model   feature :alpha 1, lambda 10\n",
    "print('사용된 feature 개수 : ',len(feats))\n",
    "print('5-fold CV f-score :',result_c[0],'average score :',np.average(result_c))\n",
    "print('predict value ratio : ',sum(comp_lab[\"pred\"]==0),sum(comp_lab[\"pred\"]==1),sum(comp_lab[\"pred\"]==2),sum(comp_lab[\"pred\"]==3))\n",
    "\n",
    "pr_rc = pd.DataFrame()\n",
    "for i in range(len(comp_lab['pred'].value_counts())):\n",
    "    prec = len(comp_lab[(comp_lab[\"pred\"]==i) & (comp_lab[\"true\"]==i)])/len(comp_lab[\"pred\"][comp_lab[\"pred\"]==i])\n",
    "    recl = len(comp_lab[(comp_lab[\"pred\"]==i) & (comp_lab[\"true\"]==i)])/len(comp_lab[\"true\"][comp_lab[\"true\"]==i])\n",
    "    print(\"precision : \",prec,\", recall : \",recl)\n",
    "    if i == 0:\n",
    "        pr_rc = pd.DataFrame({\"pred\":[prec],\"recl\":[recl]})\n",
    "    else:\n",
    "        pr_rc = pr_rc.append(pd.DataFrame({\"pred\":[prec],\"recl\":[recl]}))\n",
    "\n",
    "f_score = list()\n",
    "for i in range(len(pr_rc.index)):\n",
    "    f_score.append(1/(pr_rc[\"pred\"].tolist()[i]))\n",
    "    f_score.append(1/(pr_rc[\"recl\"].tolist()[i]))\n",
    "ls_result_lgb = 8/sum(f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp = pd.DataFrame({'name':train_all_n.columns[2:],'fi':regressor.feature_importance()}).sort_values(\"fi\",ascending=False)\n",
    "fi_zero_list = ft_imp[ft_imp.fi == 0].name.tolist()\n",
    "ft_imp.to_csv('ft_imp_02.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "feature = 'feature_02_248'\n",
    "pickle.dump(regressor,open('new_feature_'+feature+'.pickle','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

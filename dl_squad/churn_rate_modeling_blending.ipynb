{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model blending\n",
    "### Import, Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package load\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "dicpath = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_train01 = pd.read_csv('fin_train_01.csv',index_col=0)\n",
    "lgbm_train02 = pd.read_csv('fin_train_02.csv',index_col=0).sort_values('acc_id')\n",
    "lgbm_test01 = pd.read_csv('fin_test_01.csv',index_col=0)\n",
    "lgbm_test02 = pd.read_csv('fin_test_02.csv',index_col=0).sort_values('acc_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이전에 전처리한 데이터들 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modeling & blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "lgbm_train01[\"label\"] = le.fit_transform(lgbm_train01[\"label\"])\n",
    "lgbm_train02[\"label\"] = le.fit_transform(lgbm_train02[\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lgbm_train01을 활용한 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.696154\n",
      "[100]\tvalid_0's multi_logloss: 0.665882\n",
      "[150]\tvalid_0's multi_logloss: 0.655467\n",
      "[200]\tvalid_0's multi_logloss: 0.648391\n",
      "[250]\tvalid_0's multi_logloss: 0.643693\n",
      "[300]\tvalid_0's multi_logloss: 0.639974\n",
      "[350]\tvalid_0's multi_logloss: 0.636634\n",
      "[400]\tvalid_0's multi_logloss: 0.63401\n",
      "[450]\tvalid_0's multi_logloss: 0.631272\n",
      "[500]\tvalid_0's multi_logloss: 0.629455\n",
      "[550]\tvalid_0's multi_logloss: 0.627964\n",
      "[600]\tvalid_0's multi_logloss: 0.62675\n",
      "[650]\tvalid_0's multi_logloss: 0.625836\n",
      "[700]\tvalid_0's multi_logloss: 0.625018\n",
      "[750]\tvalid_0's multi_logloss: 0.624578\n",
      "[800]\tvalid_0's multi_logloss: 0.624126\n",
      "[850]\tvalid_0's multi_logloss: 0.623588\n",
      "[900]\tvalid_0's multi_logloss: 0.62323\n",
      "[950]\tvalid_0's multi_logloss: 0.622696\n",
      "[1000]\tvalid_0's multi_logloss: 0.622349\n",
      "[1050]\tvalid_0's multi_logloss: 0.622093\n",
      "[1100]\tvalid_0's multi_logloss: 0.622036\n",
      "[1150]\tvalid_0's multi_logloss: 0.622004\n",
      "[1200]\tvalid_0's multi_logloss: 0.622277\n",
      "[1250]\tvalid_0's multi_logloss: 0.622527\n",
      "[1300]\tvalid_0's multi_logloss: 0.622708\n",
      "Early stopping, best iteration is:\n",
      "[1138]\tvalid_0's multi_logloss: 0.621942\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.693869\n",
      "[100]\tvalid_0's multi_logloss: 0.664408\n",
      "[150]\tvalid_0's multi_logloss: 0.653976\n",
      "[200]\tvalid_0's multi_logloss: 0.646985\n",
      "[250]\tvalid_0's multi_logloss: 0.641288\n",
      "[300]\tvalid_0's multi_logloss: 0.637172\n",
      "[350]\tvalid_0's multi_logloss: 0.633163\n",
      "[400]\tvalid_0's multi_logloss: 0.630596\n",
      "[450]\tvalid_0's multi_logloss: 0.628106\n",
      "[500]\tvalid_0's multi_logloss: 0.626149\n",
      "[550]\tvalid_0's multi_logloss: 0.624636\n",
      "[600]\tvalid_0's multi_logloss: 0.623073\n",
      "[650]\tvalid_0's multi_logloss: 0.62208\n",
      "[700]\tvalid_0's multi_logloss: 0.621315\n",
      "[750]\tvalid_0's multi_logloss: 0.620905\n",
      "[800]\tvalid_0's multi_logloss: 0.619969\n",
      "[850]\tvalid_0's multi_logloss: 0.619266\n",
      "[900]\tvalid_0's multi_logloss: 0.618582\n",
      "[950]\tvalid_0's multi_logloss: 0.618421\n",
      "[1000]\tvalid_0's multi_logloss: 0.618355\n",
      "[1050]\tvalid_0's multi_logloss: 0.617992\n",
      "[1100]\tvalid_0's multi_logloss: 0.618091\n",
      "[1150]\tvalid_0's multi_logloss: 0.618003\n",
      "[1200]\tvalid_0's multi_logloss: 0.617997\n",
      "[1250]\tvalid_0's multi_logloss: 0.618026\n",
      "[1300]\tvalid_0's multi_logloss: 0.618116\n",
      "Early stopping, best iteration is:\n",
      "[1120]\tvalid_0's multi_logloss: 0.617874\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.695609\n",
      "[100]\tvalid_0's multi_logloss: 0.664803\n",
      "[150]\tvalid_0's multi_logloss: 0.654511\n",
      "[200]\tvalid_0's multi_logloss: 0.64732\n",
      "[250]\tvalid_0's multi_logloss: 0.641737\n",
      "[300]\tvalid_0's multi_logloss: 0.637303\n",
      "[350]\tvalid_0's multi_logloss: 0.633959\n",
      "[400]\tvalid_0's multi_logloss: 0.631438\n",
      "[450]\tvalid_0's multi_logloss: 0.628903\n",
      "[500]\tvalid_0's multi_logloss: 0.627203\n",
      "[550]\tvalid_0's multi_logloss: 0.625953\n",
      "[600]\tvalid_0's multi_logloss: 0.624562\n",
      "[650]\tvalid_0's multi_logloss: 0.623471\n",
      "[700]\tvalid_0's multi_logloss: 0.622252\n",
      "[750]\tvalid_0's multi_logloss: 0.62154\n",
      "[800]\tvalid_0's multi_logloss: 0.620812\n",
      "[850]\tvalid_0's multi_logloss: 0.6204\n",
      "[900]\tvalid_0's multi_logloss: 0.619873\n",
      "[950]\tvalid_0's multi_logloss: 0.619851\n",
      "[1000]\tvalid_0's multi_logloss: 0.619557\n",
      "[1050]\tvalid_0's multi_logloss: 0.619477\n",
      "[1100]\tvalid_0's multi_logloss: 0.619165\n",
      "[1150]\tvalid_0's multi_logloss: 0.61894\n",
      "[1200]\tvalid_0's multi_logloss: 0.618626\n",
      "[1250]\tvalid_0's multi_logloss: 0.618593\n",
      "[1300]\tvalid_0's multi_logloss: 0.618618\n",
      "[1350]\tvalid_0's multi_logloss: 0.618871\n",
      "[1400]\tvalid_0's multi_logloss: 0.619093\n",
      "[1450]\tvalid_0's multi_logloss: 0.61925\n",
      "Early stopping, best iteration is:\n",
      "[1265]\tvalid_0's multi_logloss: 0.618465\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.704029\n",
      "[100]\tvalid_0's multi_logloss: 0.673241\n",
      "[150]\tvalid_0's multi_logloss: 0.662112\n",
      "[200]\tvalid_0's multi_logloss: 0.654613\n",
      "[250]\tvalid_0's multi_logloss: 0.64858\n",
      "[300]\tvalid_0's multi_logloss: 0.643941\n",
      "[350]\tvalid_0's multi_logloss: 0.640256\n",
      "[400]\tvalid_0's multi_logloss: 0.637707\n",
      "[450]\tvalid_0's multi_logloss: 0.634564\n",
      "[500]\tvalid_0's multi_logloss: 0.631966\n",
      "[550]\tvalid_0's multi_logloss: 0.629996\n",
      "[600]\tvalid_0's multi_logloss: 0.628542\n",
      "[650]\tvalid_0's multi_logloss: 0.627168\n",
      "[700]\tvalid_0's multi_logloss: 0.625866\n",
      "[750]\tvalid_0's multi_logloss: 0.624988\n",
      "[800]\tvalid_0's multi_logloss: 0.624533\n",
      "[850]\tvalid_0's multi_logloss: 0.623535\n",
      "[900]\tvalid_0's multi_logloss: 0.622762\n",
      "[950]\tvalid_0's multi_logloss: 0.622373\n",
      "[1000]\tvalid_0's multi_logloss: 0.622086\n",
      "[1050]\tvalid_0's multi_logloss: 0.621859\n",
      "[1100]\tvalid_0's multi_logloss: 0.621573\n",
      "[1150]\tvalid_0's multi_logloss: 0.621489\n",
      "[1200]\tvalid_0's multi_logloss: 0.621447\n",
      "[1250]\tvalid_0's multi_logloss: 0.621689\n",
      "[1300]\tvalid_0's multi_logloss: 0.621882\n",
      "[1350]\tvalid_0's multi_logloss: 0.621987\n",
      "[1400]\tvalid_0's multi_logloss: 0.621979\n",
      "Early stopping, best iteration is:\n",
      "[1203]\tvalid_0's multi_logloss: 0.621333\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.709788\n",
      "[100]\tvalid_0's multi_logloss: 0.679238\n",
      "[150]\tvalid_0's multi_logloss: 0.66927\n",
      "[200]\tvalid_0's multi_logloss: 0.662412\n",
      "[250]\tvalid_0's multi_logloss: 0.657419\n",
      "[300]\tvalid_0's multi_logloss: 0.652986\n",
      "[350]\tvalid_0's multi_logloss: 0.649889\n",
      "[400]\tvalid_0's multi_logloss: 0.647335\n",
      "[450]\tvalid_0's multi_logloss: 0.645164\n",
      "[500]\tvalid_0's multi_logloss: 0.643084\n",
      "[550]\tvalid_0's multi_logloss: 0.641427\n",
      "[600]\tvalid_0's multi_logloss: 0.640307\n",
      "[650]\tvalid_0's multi_logloss: 0.639\n",
      "[700]\tvalid_0's multi_logloss: 0.637923\n",
      "[750]\tvalid_0's multi_logloss: 0.637157\n",
      "[800]\tvalid_0's multi_logloss: 0.6366\n",
      "[850]\tvalid_0's multi_logloss: 0.636257\n",
      "[900]\tvalid_0's multi_logloss: 0.635578\n",
      "[950]\tvalid_0's multi_logloss: 0.635306\n",
      "[1000]\tvalid_0's multi_logloss: 0.635013\n",
      "[1050]\tvalid_0's multi_logloss: 0.63473\n",
      "[1100]\tvalid_0's multi_logloss: 0.634544\n",
      "[1150]\tvalid_0's multi_logloss: 0.634404\n",
      "[1200]\tvalid_0's multi_logloss: 0.634462\n",
      "[1250]\tvalid_0's multi_logloss: 0.634598\n",
      "[1300]\tvalid_0's multi_logloss: 0.634791\n",
      "[1350]\tvalid_0's multi_logloss: 0.635207\n",
      "Early stopping, best iteration is:\n",
      "[1161]\tvalid_0's multi_logloss: 0.634314\n",
      "CPU times: user 1h 9min 37s, sys: 7min 8s, total: 1h 16min 45s\n",
      "Wall time: 18min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "# Create arrays and dataframes to store results\n",
    "feats1 = [f for f in lgbm_train01.columns if f not in ['label','acc_id']]\n",
    "feats2 = [f for f in lgbm_train02.columns if f not in ['label','acc_id']]\n",
    "\n",
    "folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "\n",
    "result_e = []\n",
    "preds= []\n",
    "\n",
    "num_trials = 1\n",
    "num_rounds = 3000\n",
    "early_stopping_rounds = 200\n",
    "params = {\n",
    "    'objective':['multiclass'],\n",
    "    'num_class':[4],\n",
    "    'class_weight':['balanced'],\n",
    "    'boosting':['gbdt'],\n",
    "    #'min_child_weight': list(np.arange(1,20,1)),\n",
    "    'colsample_bytree': [0.6],\n",
    "    #'max_depth': list(np.arange(3,15,1)),\n",
    "    #'subsample': list(np.arange(0.4,1,0.1))\n",
    "    'reg_lambda': [10],\n",
    "    'reg_alpha' : [1]\n",
    "    #'learning_rate': [0.005,0.01,0.05,0.1],\n",
    "    #'num_leaves' :list(np.arange(20,55,3)),\n",
    "}\n",
    "selected_params = []\n",
    "\n",
    "for trial, param in zip(np.arange(num_trials),list(ParameterSampler(params, n_iter=num_trials))):\n",
    "    result_b = []\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(lgbm_train01[feats1], lgbm_train01['label'])):\n",
    "    \n",
    "        #if n_fold != 0:\n",
    "        #    break\n",
    "        \n",
    "        train_x, train_y = lgbm_train01[feats1].iloc[train_idx], lgbm_train01['label'].iloc[train_idx]\n",
    "        valid_x, valid_y = lgbm_train01[feats1].iloc[valid_idx], lgbm_train01['label'].iloc[valid_idx] \n",
    "        \n",
    "\n",
    "        lgb_train = lgb.Dataset(train_x, train_y)\n",
    "        lgb_eval = lgb.Dataset(valid_x, valid_y, reference=lgb_train)\n",
    "        \n",
    "        regressor01 = lgb.train(param,\n",
    "                              lgb_train,\n",
    "                              valid_sets = lgb_eval,\n",
    "                              num_boost_round = num_rounds,\n",
    "                              verbose_eval=50,\n",
    "                              early_stopping_rounds=early_stopping_rounds\n",
    "                             )\n",
    "\n",
    "        y_pred = regressor01.predict(valid_x)\n",
    "        preds.append(y_pred)\n",
    "\n",
    "        max_pred = []\n",
    "        for i in range(len(y_pred)):\n",
    "            max_pred.append(np.argmax(y_pred[i]))\n",
    "\n",
    "        comp_lab = pd.DataFrame({'pred':max_pred,'true':valid_y})\n",
    "\n",
    "        pr_rc = pd.DataFrame()\n",
    "        for i in range(len(comp_lab['pred'].value_counts())):\n",
    "            prec = len(comp_lab[(comp_lab[\"pred\"]==i) & (comp_lab[\"true\"]==i)])/len(comp_lab[\"pred\"][comp_lab[\"pred\"]==i])\n",
    "            recl = len(comp_lab[(comp_lab[\"pred\"]==i) & (comp_lab[\"true\"]==i)])/len(comp_lab[\"true\"][comp_lab[\"true\"]==i])\n",
    "\n",
    "            if i == 0:\n",
    "                pr_rc = pd.DataFrame({\"pred\":[prec],\"recl\":[recl]})\n",
    "            else:\n",
    "                pr_rc = pr_rc.append(pd.DataFrame({\"pred\":[prec],\"recl\":[recl]}))\n",
    "\n",
    "        f_score = list()\n",
    "        for i in range(len(pr_rc.index)):\n",
    "            f_score.append(1/(pr_rc[\"pred\"].tolist()[i]))\n",
    "            f_score.append(1/(pr_rc[\"recl\"].tolist()[i]))\n",
    "        ls_result_lgb = 8/sum(f_score)\n",
    "        \n",
    "        result_b.append(ls_result_lgb)\n",
    "        \n",
    "        \n",
    "    selected_params.append(param)\n",
    "    result_e.append(result_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lgbm_train02을 활용한 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.661005\n",
      "[200]\tvalid_0's multi_logloss: 0.639819\n",
      "[300]\tvalid_0's multi_logloss: 0.627973\n",
      "[400]\tvalid_0's multi_logloss: 0.621113\n",
      "[500]\tvalid_0's multi_logloss: 0.616033\n",
      "[600]\tvalid_0's multi_logloss: 0.612186\n",
      "[700]\tvalid_0's multi_logloss: 0.60964\n",
      "[800]\tvalid_0's multi_logloss: 0.607602\n",
      "[900]\tvalid_0's multi_logloss: 0.605954\n",
      "[1000]\tvalid_0's multi_logloss: 0.604797\n",
      "[1100]\tvalid_0's multi_logloss: 0.604165\n",
      "[1200]\tvalid_0's multi_logloss: 0.604013\n",
      "[1300]\tvalid_0's multi_logloss: 0.604075\n",
      "[1400]\tvalid_0's multi_logloss: 0.604314\n",
      "Early stopping, best iteration is:\n",
      "[1272]\tvalid_0's multi_logloss: 0.603782\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.659816\n",
      "[200]\tvalid_0's multi_logloss: 0.637685\n",
      "[300]\tvalid_0's multi_logloss: 0.625512\n",
      "[400]\tvalid_0's multi_logloss: 0.6176\n",
      "[500]\tvalid_0's multi_logloss: 0.611607\n",
      "[600]\tvalid_0's multi_logloss: 0.607686\n",
      "[700]\tvalid_0's multi_logloss: 0.604554\n",
      "[800]\tvalid_0's multi_logloss: 0.603059\n",
      "[900]\tvalid_0's multi_logloss: 0.601981\n",
      "[1000]\tvalid_0's multi_logloss: 0.601226\n",
      "[1100]\tvalid_0's multi_logloss: 0.600731\n",
      "[1200]\tvalid_0's multi_logloss: 0.600434\n",
      "[1300]\tvalid_0's multi_logloss: 0.600342\n",
      "[1400]\tvalid_0's multi_logloss: 0.60076\n",
      "Early stopping, best iteration is:\n",
      "[1299]\tvalid_0's multi_logloss: 0.60032\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.656841\n",
      "[200]\tvalid_0's multi_logloss: 0.633621\n",
      "[300]\tvalid_0's multi_logloss: 0.621905\n",
      "[400]\tvalid_0's multi_logloss: 0.6141\n",
      "[500]\tvalid_0's multi_logloss: 0.608834\n",
      "[600]\tvalid_0's multi_logloss: 0.605056\n",
      "[700]\tvalid_0's multi_logloss: 0.602097\n",
      "[800]\tvalid_0's multi_logloss: 0.59985\n",
      "[900]\tvalid_0's multi_logloss: 0.598553\n",
      "[1000]\tvalid_0's multi_logloss: 0.598116\n",
      "[1100]\tvalid_0's multi_logloss: 0.597531\n",
      "[1200]\tvalid_0's multi_logloss: 0.597079\n",
      "[1300]\tvalid_0's multi_logloss: 0.597424\n",
      "Early stopping, best iteration is:\n",
      "[1195]\tvalid_0's multi_logloss: 0.597052\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.668289\n",
      "[200]\tvalid_0's multi_logloss: 0.646159\n",
      "[300]\tvalid_0's multi_logloss: 0.634343\n",
      "[400]\tvalid_0's multi_logloss: 0.627098\n",
      "[500]\tvalid_0's multi_logloss: 0.621579\n",
      "[600]\tvalid_0's multi_logloss: 0.617072\n",
      "[700]\tvalid_0's multi_logloss: 0.614229\n",
      "[800]\tvalid_0's multi_logloss: 0.611956\n",
      "[900]\tvalid_0's multi_logloss: 0.610029\n",
      "[1000]\tvalid_0's multi_logloss: 0.608776\n",
      "[1100]\tvalid_0's multi_logloss: 0.60811\n",
      "[1200]\tvalid_0's multi_logloss: 0.607367\n",
      "[1300]\tvalid_0's multi_logloss: 0.606911\n",
      "[1400]\tvalid_0's multi_logloss: 0.606689\n",
      "[1500]\tvalid_0's multi_logloss: 0.607217\n",
      "Early stopping, best iteration is:\n",
      "[1383]\tvalid_0's multi_logloss: 0.606565\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.673012\n",
      "[200]\tvalid_0's multi_logloss: 0.651414\n",
      "[300]\tvalid_0's multi_logloss: 0.639802\n",
      "[400]\tvalid_0's multi_logloss: 0.632007\n",
      "[500]\tvalid_0's multi_logloss: 0.626887\n",
      "[600]\tvalid_0's multi_logloss: 0.623039\n",
      "[700]\tvalid_0's multi_logloss: 0.620136\n",
      "[800]\tvalid_0's multi_logloss: 0.618022\n",
      "[900]\tvalid_0's multi_logloss: 0.616519\n",
      "[1000]\tvalid_0's multi_logloss: 0.61572\n",
      "[1100]\tvalid_0's multi_logloss: 0.615315\n",
      "[1200]\tvalid_0's multi_logloss: 0.615115\n",
      "[1300]\tvalid_0's multi_logloss: 0.615163\n",
      "[1400]\tvalid_0's multi_logloss: 0.615371\n",
      "Early stopping, best iteration is:\n",
      "[1239]\tvalid_0's multi_logloss: 0.614901\n",
      "CPU times: user 46min 1s, sys: 5min 41s, total: 51min 43s\n",
      "Wall time: 12min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds2 = []\n",
    "\n",
    "for trial, param in zip(np.arange(num_trials),list(ParameterSampler(params, n_iter=num_trials))):\n",
    "    result_b = []\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(lgbm_train02[feats2], lgbm_train02['label'])):\n",
    "        \n",
    "        #if n_fold != 0:\n",
    "        #    break\n",
    "        #model 02 light gbm with 306\n",
    "        train_x, train_y = lgbm_train02[feats2].iloc[train_idx], lgbm_train02['label'].iloc[train_idx]\n",
    "        valid_x, valid_y2 = lgbm_train02[feats2].iloc[valid_idx], lgbm_train02['label'].iloc[valid_idx] \n",
    "\n",
    "\n",
    "        lgb_train = lgb.Dataset(train_x, train_y)\n",
    "        lgb_eval = lgb.Dataset(valid_x, valid_y2, reference=lgb_train)\n",
    "\n",
    "        regressor02 = lgb.train(param,\n",
    "                              lgb_train,\n",
    "                              valid_sets = lgb_eval,\n",
    "                              num_boost_round = num_rounds,\n",
    "                              verbose_eval=100,\n",
    "                              early_stopping_rounds=early_stopping_rounds\n",
    "                             )\n",
    "        \n",
    "        y_pred = regressor02.predict(valid_x)\n",
    "        preds2.append(y_pred)\n",
    "        \n",
    "        \n",
    "        max_pred = []\n",
    "        for i in range(len(y_pred)):\n",
    "            max_pred.append(np.argmax(y_pred[i]))\n",
    "\n",
    "        comp_lab = pd.DataFrame({'pred':max_pred,'true':valid_y2})\n",
    "\n",
    "        pr_rc = pd.DataFrame()\n",
    "        for i in range(len(comp_lab['pred'].value_counts())):\n",
    "            prec = len(comp_lab[(comp_lab[\"pred\"]==i) & (comp_lab[\"true\"]==i)])/len(comp_lab[\"pred\"][comp_lab[\"pred\"]==i])\n",
    "            recl = len(comp_lab[(comp_lab[\"pred\"]==i) & (comp_lab[\"true\"]==i)])/len(comp_lab[\"true\"][comp_lab[\"true\"]==i])\n",
    "\n",
    "            if i == 0:\n",
    "                pr_rc = pd.DataFrame({\"pred\":[prec],\"recl\":[recl]})\n",
    "            else:\n",
    "                pr_rc = pr_rc.append(pd.DataFrame({\"pred\":[prec],\"recl\":[recl]}))\n",
    "\n",
    "        f_score = list()\n",
    "        for i in range(len(pr_rc.index)):\n",
    "            f_score.append(1/(pr_rc[\"pred\"].tolist()[i]))\n",
    "            f_score.append(1/(pr_rc[\"recl\"].tolist()[i]))\n",
    "        ls_result_lgb = 8/sum(f_score)\n",
    "        \n",
    "        result_b.append(ls_result_lgb)\n",
    "        \n",
    "        \n",
    "    selected_params.append(param)\n",
    "    result_e.append(result_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-fold cv 에서 각각 lgbm_train01/02 모델링 결과 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7160195098593874,\n",
       "  0.7137735765609082,\n",
       "  0.7143850862660739,\n",
       "  0.7074211596942418,\n",
       "  0.7044487240688693],\n",
       " [0.7210618279558735,\n",
       "  0.7227018623512634,\n",
       "  0.7236471857559741,\n",
       "  0.7214727934597943,\n",
       "  0.7166287761749196]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 두 모델의 예측값(softmax function 결과)의 평균\n",
    "- 검증셋에서의 결과로 대략적 비율을 정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 5421 3863 5398 5318\n",
      "true: 5031 4979 4966 5024\n",
      "precision :  0.6342003320420586 , recall :  0.6833631484794276\n",
      "precision :  0.7077400983691432 , recall :  0.5491062462341836\n",
      "precision :  0.774546128195628 , recall :  0.8419250906161901\n",
      "precision :  0.8544565626175253 , recall :  0.9044585987261147\n",
      "0.7253584412252198\n",
      "------------------------------------\n",
      "pred: 5387 4009 5295 5309\n",
      "true: 5076 4923 4970 5031\n",
      "precision :  0.6441433079636161 , recall :  0.6836091410559496\n",
      "precision :  0.6852082813669245 , recall :  0.5579930936420882\n",
      "precision :  0.7852691218130312 , recall :  0.8366197183098592\n",
      "precision :  0.8560934262572989 , recall :  0.9033989266547406\n",
      "0.7265736716516179\n",
      "------------------------------------\n",
      "pred: 5646 3751 5281 5322\n",
      "true: 4959 5023 4980 5038\n",
      "precision :  0.6234502302515055 , recall :  0.7098205283323251\n",
      "precision :  0.714476139696081 , recall :  0.5335456898267967\n",
      "precision :  0.783753077068737 , recall :  0.8311244979919679\n",
      "precision :  0.8526869597895528 , recall :  0.9007542675664947\n",
      "0.7240664328618973\n",
      "------------------------------------\n",
      "pred: 5445 3933 5340 5282\n",
      "true: 4904 5021 5025 5050\n",
      "precision :  0.6224058769513315 , recall :  0.691068515497553\n",
      "precision :  0.7065853038393084 , recall :  0.5534754033061143\n",
      "precision :  0.7838951310861423 , recall :  0.8330348258706468\n",
      "precision :  0.8549791745550928 , recall :  0.8942574257425743\n",
      "0.7246572626119945\n",
      "------------------------------------\n",
      "pred: 5428 4030 5373 5169\n",
      "true: 5030 5054 5059 4857\n",
      "precision :  0.628960943257185 , recall :  0.6787276341948311\n",
      "precision :  0.6873449131513648 , recall :  0.5480807281361298\n",
      "precision :  0.7807556300018612 , recall :  0.829215259932793\n",
      "precision :  0.845037724898433 , recall :  0.8993205682520075\n",
      "0.7191146390698493\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "temp_01 = []\n",
    "for i in enumerate(folds.split(lgbm_train01)):\n",
    "    temp_01.append(i)\n",
    "\n",
    "vaild_tmp1 = lgbm_train02['label'].iloc[temp_01[0][1][1]]\n",
    "vaild_tmp2 = lgbm_train02['label'].iloc[temp_01[1][1][1]]\n",
    "vaild_tmp3 = lgbm_train02['label'].iloc[temp_01[2][1][1]]\n",
    "vaild_tmp4 = lgbm_train02['label'].iloc[temp_01[3][1][1]]\n",
    "vaild_tmp5 = lgbm_train02['label'].iloc[temp_01[4][1][1]]\n",
    "\n",
    "tmps = [vaild_tmp1,vaild_tmp2,vaild_tmp3,vaild_tmp4,vaild_tmp5]\n",
    "\n",
    "for j in range(5): \n",
    "    a = [preds[j],preds2[j]]\n",
    "    max_pred_avg =  []   \n",
    "    b = np.average(a, axis=0).tolist()\n",
    "\n",
    "    for i in range(len(b)):\n",
    "        max_pred_avg.append(np.argmax(b[i]))  \n",
    "\n",
    "    comp_lab = pd.DataFrame({'pred':max_pred_avg,'true':tmps[j]})\n",
    "    comp_lab[\"pred\"] = comp_lab[\"pred\"].astype(int)\n",
    "    print('pred:',sum(comp_lab[\"pred\"]==0),sum(comp_lab[\"pred\"]==1),sum(comp_lab[\"pred\"]==2),sum(comp_lab[\"pred\"]==3))\n",
    "    print('true:',sum(comp_lab[\"true\"]==0),sum(comp_lab[\"true\"]==1),sum(comp_lab[\"true\"]==2),sum(comp_lab[\"true\"]==3))\n",
    "\n",
    "    #comp_lab = pd.DataFrame({'pred':max_pred,'true':valid_y})\n",
    "\n",
    "    pr_rc = pd.DataFrame()\n",
    "    for i in range(len(comp_lab['pred'].value_counts())):\n",
    "        prec = len(comp_lab[(comp_lab[\"pred\"]==i) & (comp_lab[\"true\"]==i)])/len(comp_lab[\"pred\"][comp_lab[\"pred\"]==i])\n",
    "        recl = len(comp_lab[(comp_lab[\"pred\"]==i) & (comp_lab[\"true\"]==i)])/len(comp_lab[\"true\"][comp_lab[\"true\"]==i])\n",
    "        print(\"precision : \",prec,\", recall : \",recl)\n",
    "        if i == 0:\n",
    "            pr_rc = pd.DataFrame({\"pred\":[prec],\"recl\":[recl]})\n",
    "        else:\n",
    "            pr_rc = pr_rc.append(pd.DataFrame({\"pred\":[prec],\"recl\":[recl]}))\n",
    "\n",
    "    f_score = list()\n",
    "    for i in range(len(pr_rc.index)):\n",
    "        f_score.append(1/(pr_rc[\"pred\"].tolist()[i]))\n",
    "        f_score.append(1/(pr_rc[\"recl\"].tolist()[i]))\n",
    "    ls_result_lgb = 8/sum(f_score)\n",
    "    print(ls_result_lgb)\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "2018/08/09\n",
    "feature engineering 과정이 한참 더 필요(EDA도 필요함)\n",
    "    -시리얼데이터 어떻게 반영해햐니\n",
    "    -노트에 반영된 아이디어 구현하기\n",
    "    \n",
    "multiclass를 어떻게 핸들링해야할지(loss function??)\n",
    "\n",
    "2018/08/10\n",
    "조금더 향상된 성능 & cv에서도 안정적으로 결과가 나옴\n",
    "파라미터 서치는 따로 진행하지 않고 3fold cv를 learning_rate : 0.05, num_rounds : 2000 으로 진행\n",
    "\n",
    "1순위 : multi_logloss를 loss function으로 사용하였는데, \n",
    "        01) 문제를 0.25, 0.5, 0.75, 1 형태로 레이블을 바꾼다음 logistic형태로 풀어볼까?\n",
    "        02)\n",
    "2순위 : feature을 경우 아직 train_party, train_trade를 사용하지 않음 + 노트에 정리된 것도 반영X\n",
    "\n",
    "\n",
    "\n",
    "2018/08/13\n",
    "1차 제출 : 0.6896 (23/46)\n",
    "이전에 만든 코드를 test set에 적용해서 나온 결과\n",
    "파라미터 서치는 따로 진행하지 않고 3fold cv를 learning_rate : 0.05, num_rounds : 2000 으로 진행\n",
    "\n",
    "1순위 : multi_logloss를 loss function으로 사용하였는데, \n",
    "        01) 문제를 0.25, 0.5, 0.75, 1 형태로 레이블을 바꾼다음 logistic형태로 풀어볼까?\n",
    "        02) multi_auroc 사용 -> multi_auroc code 작성하기\n",
    "        \n",
    "2순위 : feature을 경우 아직 train_party, train_trade를 사용하지 않음 + 노트에 정리된 것도 반영X\n",
    "        01) 파생변수 생성\n",
    "        02) 특성 선택 방법 ( 모델기반[feature importance]특성 선택, data_leakage 체크, 상관관계 체크)\n",
    "\n",
    "3순위 : 결과의 threshhold??, hyper parameter??, 다른 모델 활용(xgboost, nn, lr 등등)\n",
    "\n",
    "2018/08/14\n",
    "[0.6947982139926345, 0.6901791726318248, 0.6818321058701344] <- 기존\n",
    "\n",
    "[0.7252078093749054, 0.7369634579247871, 0.7334708110879647] <- 1순위 01)\n",
    "실제 제출 결과(2차 제출)는 0.5644 threshhold의 문제로 보임 'week'만 15000개... \n",
    "\n",
    "[0.6727887000512414, 0.642219781355088, 0.6692332969310869] <- 1순위 02)\n",
    "auroc의 경우 성능이 multi_logloss에 비해 하락. 나중에 시간이 되면 다시 코드에 이상이 없는지 체크 일단 패스\n",
    "\n",
    "1순위 : multi_logloss를 loss function으로 사용하였는데, \n",
    "        01) 문제를 1,2,3,4 형태로 레이블을 바꾼다음 logistic형태로 풀어볼까?\n",
    "        02) multi_auroc 사용 -> multi_auroc code 작성하기\n",
    "        03) 다른 방법.. f1 해보기\n",
    "        \n",
    "2순위 : feature을 경우 아직 train_party, train_trade를 사용하지 않음 + 노트에 정리된 것도 반영X\n",
    "        01) 파생변수 생성\n",
    "        02) 특성 선택 방법 ( 모델기반[feature importance]특성 선택, data_leakage 체크, 상관관계 체크)\n",
    "\n",
    "3순위 : 결과의 threshhold??, hyper parameter??, 다른 모델 활용(xgboost, nn, lr 등등)\n",
    "\n",
    "2018/08/16\n",
    "3차 제출 : 0.6967 (26/60)\n",
    "- f1 score를 사용하였는데 제대로 업데이트가 안되는(0.5수준에서 시작하여 미미한 향상)것을 발견.. 어떤부분이 문제인지 파악 못함\n",
    "    => 일단 multi-logloss를 사용해서 진행(1순위 변경)\n",
    "- feature생성 계획\n",
    "    생성 => 모델링 => feature importance 확인 => 가지치기 => 생성 => 모델링...\n",
    "    현재 feature importanve에서는 예상대로 play time이 가장 주요한 변수로 나타남\n",
    "    생성방법 : feature tools를 이용한 영혼없는 생성, 가설 설정과 그에 기반한 변수 생성\n",
    "\n",
    "- issue 이메일 문의 결과\n",
    "\"\"\"\n",
    "'train_trade.csv' 에서 유저들의 거래 기록을 보고 있는데, 배경 지식에 기반하여 생각할 때 \n",
    "\n",
    "거래는 소스유저(source_acc_id) 와 타겟유저(target_acc_id)간의 상호간의 작용이 대부분이라고 판단하였습니다.\n",
    "(A유저가 ㄱ아이템을 B에게 주면 B유저는 그에 상응하는 재화나 ㄴ아이템을 주는 방식. 물론 일방향의 거래도 \n",
    "발생할 수 있지만 가능성은 전자의 경우가 많을 것으로 판단)\n",
    "\n",
    "그런데 거래 주, 일, 시간별로 sorting해서 살펴본 결과 상호간의 거래는 거의 없고 일방향으로 거래를 하는 것으로 보여져서 이에 대해 \n",
    "1. 데이터가 샘플링된 것인지 2. 거래 시스템상 정상적으로 발생된 데이터인지 궁금하여 질문을 남깁니다.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------\n",
    "<엔씨소프트  답변>\n",
    " \n",
    "실제로 일방향의 거래가 대부분임.\n",
    "추가로, 거래 데이터의 경우 추출 오류로인해 중복 집계된 데이터 존재함. \n",
    "이를 감안하여 분석 진행하기 바람.\n",
    "----------------------------------------------------------------------------------------------------\n",
    "a. 일방향 형태의 거래가 더 많다\n",
    "b. 중복집계된 데이터가 존재\n",
    "\n",
    "a') 순수하게 user to user로 기부? 형태의 거래가 많이 발생하는지, 현금 거래처럼 게임 외부에서의 거래 이후 거래가 발생하는지,\n",
    "    경매장 시스템으로 인해 일방향의 거래만 집계가 되는지\n",
    "        => 1:1 교환 건 수가 없기 때문에 1,2번 형태는 아닌거 같음... 실제로 인게임에서 어떻게 거래가 형성되고 있는지 알아보기\n",
    "        위해 직접 육성 중...\n",
    "        \n",
    "b') 중복 집계된 데이터는 unique를 통해 제거할 필요가 있음(날짜+시간+아이템+수량정도로 묶어서 중복을 제거하면 될듯)\n",
    "        => 제거완료(데이터 사이즈가 절반 가까이 소멸.. nc 당신은 대체...)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "- 30*(mean,max,min =3)여개의 신규 feature 생성\n",
    "    activity 데이터 1차 완료\n",
    "\n",
    "- 파티와 거래데이터 활용방법에 대한 아이디어 필요\n",
    "    지하철에서 생각하기\n",
    "    \n",
    "\n",
    "2018/08/17\n",
    "작업 구분\n",
    "A : loss function, parameter search...\n",
    "B : data EDA(feature importance check, data leakage, correlation...)\n",
    "C : feature engineering\n",
    "D : model selection, blending\n",
    "\n",
    "오늘 주요 작업 B, C.\n",
    "- activity 이외의 변수들로 변수 생성하기\n",
    "\n",
    "- 상관관계 체크\n",
    "\n",
    "아이디어\n",
    "-비율로 feature를 생성하는 것 말고 다른 방식을 생각해보기\n",
    "-table 간의 cross feature\n",
    "-featuretools 에서 더 다양한 agg 방식을 적용해보기\n",
    "\n",
    "\n",
    "2018/08/19\n",
    "주요 작업 B, C\n",
    "- trade, party변수에 대한 idea 정리하기 & 변수 생성\n",
    "\n",
    "- 상관관계 확인 ( data leakage 체크는 할 필요 없는듯 <= 베이스 모델에서 cv평균이랑 리더보드 결과랑 차이 0.001 이하)\n",
    "\n",
    "아이디어\n",
    "-비율로 feature를 생성하는 것 말고 다른 방식을 생각해보기\n",
    "-table 간의 cross feature\n",
    "-featuretools 에서 더 다양한 agg 방식을 적용해보기\n",
    "\n",
    "2018/08/20\n",
    "주요 작업 B, C\n",
    "- trade변수로는 파생변수 생성완료\n",
    "- party는 인원이 1~60이상으로 다양하게 나옴 좀더 생각해보고 파생변수를 만들어야 할듯\n",
    "- 수요일 이내로 1차 파생변수 생성은 마무리하고 best single model 찾기(BO) -> 시작은 lgb로.. 8월내로 1차 마무리\n",
    "- 9월 첫주 : 블렌딩 or stacking 으로 최적의 조합찾기(BO활용)\n",
    "- 9월 둘쨋주 : 버퍼\n",
    "\n",
    "아이디어\n",
    "-비율이 같은 경우라면 선형으로 예측한뒤 valid셋의 비율만큼 threshhold정하기??\n",
    "-비율로 feature를 생성하는 것 말고 다른 방식을 생각해보기\n",
    "-table 간의 cross feature\n",
    "-featuretools 에서 더 다양한 agg 방식을 적용해보기\n",
    "\n",
    "2018/08/23\n",
    "B,C 작업 1단계 완료 -> 추후에 feature 더 생성할때 추가하기 이제 그만 \n",
    "cv5fold 성능 평균 : 0.7073105337940665\n",
    "\n",
    "상위 feature importance\n",
    "play_time의 std\n",
    "play_time의 평균\n",
    "cnt_dt의 std\n",
    "play_time의 max\n",
    "cnt_dt의 평균\n",
    "\n",
    "\n",
    "주요 작업 A\n",
    "random search와 Bayesian optimization을 이용하고 비교해보기\n",
    "\n",
    "\n",
    "할것\n",
    "-피클 저장(all_ft_np,filt_ft_np)\n",
    "-random search, Bayesian optimization\n",
    "-neural net으로 모델만들기(dnn, rnn이 가능성있을듯?)\n",
    "\n",
    "아이디어\n",
    "-블렌딩에 쓸 feature들로 각 주의 값들(총 8개가 될 듯)을 사용??, weight도 시간별로 줄 수 있을듯?\n",
    "\n",
    "2018/08/26\n",
    "\n",
    "-Bayesian optimization에서 lossfunction customize 하는 법을 알지 못해... parameter search로 변경\n",
    "(시간도 BO에서 약 19시간+가 걸림...)\n",
    "-뉴럴넷에 쓸 수 있도록 파일 가공하기\n",
    "\n",
    "2018/09/03\n",
    "reg_lambda = 10\n",
    "col.. = 0.8\n",
    "\n",
    "\n",
    "2018/09/05\n",
    "파생변수 생성중...\n",
    "+dnn model도 돌리는즁\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

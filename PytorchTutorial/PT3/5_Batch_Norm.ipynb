{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T00:50:07.322573Z",
     "start_time": "2018-07-23T00:50:06.849694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1108a1b10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as vdatasets\n",
    "import torchvision.utils as vutils\n",
    "from tensorboardX import SummaryWriter\n",
    "import pickle,os,shutil\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Batch Normalization\n",
    "- batch normalization은 기본적으로 gradient vanishing/gradient exploding이 일어나지 않도록 하는 아이디어 중 하나임.\n",
    "- 학습이 불안정한 이유는 'internal Covariance shift' 때문이라는 의견이 주장됨.\n",
    "- Covariance shift는 network의 각 층이나 activation마다 input의 distribution이 달라지는 현상을 의미함. (batch마다 분포가 달라진다?)\n",
    "- 이 현상을 막기 위해 간단하게 각 층의 input의 distribution의 평균을 0, 표준편차를 1인 input으로 normalize시키는 방법 -> 'whitening'\n",
    "- 'whitening'을 하기 위해서는 covariance matrix의 계산과 inverse계산이 필요하기 때문에 연산량이 많음.\n",
    "- whitening의 단점을 보완하고 internal covariance shift를 줄이기 위해 다음과 같이 접근함.  \n",
    "  \n",
    "    - 각각의 feature들이 이미 uncorrelated라고 가정하고, feature각각에 대해서만 scalar형태로 mean, variance를 구하고 각각 normalize\n",
    "    - 단순히 mean과 variance를 0, 1로 고정시키는 것은 오히려 activation의 nonlinearity를 없앨 수 있어서 normalize된 값들에 scale factor(gamma)와 shift(beta)를 더해주고 이 변수들을 back-prop과정에서 같이 train시킴.\n",
    "    - training data전체에 대해 mean과 variance를 구하는 것이 아니라, mini-batch단위로 접근하여 계산함.\n",
    "    \n",
    "<img src='./images/batchnorm_alg.png' width =400 align='left'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실제로 BatchNormalization은 네트워크에 적용시킬 때는, activation function을 하기 전에 적용한다.  \n",
    "\n",
    "<img src='./images/batchnorm_feed.png' width =400 align='left'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training할 때는 mini-batch에서 평균과 표준편차를 이용하지만 test data를 사용하여 inference할 때는 training할 때 현자까지 본 input들의 이동평균 및 unbiased variance estimate의 이동평균을 계산해 저장해 놓은 뒤 이 값으로 normalize를 함. \n",
    "\n",
    "\n",
    "## BatchNormalization의 장점\n",
    "- 기존 Deep Neural Network에서 leraning rate를 크게 잡을 경우 gradient가 explode/vanish 등의 문제가 발생함. 이는 parameter의 scale때문인데, batchnorm을 사용할 경우 back-prop할 때 scale에 영항을 받지 않게 된다. 따라서, learning rate를 크게 잡을 수 있게 되고 이는 빠른 학습을 가능하게 함.\n",
    "- Batch Normalization은 Regularization효과가 있음. (drop out효과와 같으며 weight regularization term등을 제외할 수 있음.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 텐서보드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T04:18:09.099057Z",
     "start_time": "2018-07-23T04:18:09.093530Z"
    }
   },
   "outputs": [],
   "source": [
    "port= '6006'\n",
    "try:\n",
    "    shutil.rmtree('runs/')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T04:18:38.205171Z",
     "start_time": "2018-07-23T04:18:38.084059Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE= 64\n",
    "\n",
    "train_dataset = vdatasets.MNIST(root='../data/MNIST/',\n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2,\n",
    "                                           drop_last=True) # 이동평균이 튀는걸 방지\n",
    "\n",
    "test_dataset = vdatasets.MNIST(root='../data/MNIST/',\n",
    "                               train=False, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T04:35:12.094688Z",
     "start_time": "2018-07-23T04:35:12.079590Z"
    }
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.linear1= nn.Linear(input_size, hidden_size)\n",
    "        self.linear2= nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3= nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # In : {배치사이즈, 차원수} => Out : (배치사이즈, 차원수)\n",
    "        self.bn1= nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2= nn.BatchNorm1d(hidden_size)\n",
    "    def forward(self, inputs):\n",
    "        outputs= self.bn1(self.linear1(inputs))\n",
    "        outputs= F.relu(outputs)\n",
    "        outputs= self.bn2(self.linear2(outputs))\n",
    "        outputs= F.relu(outputs)\n",
    "        return self.linear3(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T04:35:12.721833Z",
     "start_time": "2018-07-23T04:35:12.710089Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCH=5\n",
    "LR=0.1\n",
    "\n",
    "INPUT_SIZE= 784\n",
    "HIDDEN_SIZE= 512\n",
    "OUTPUT_SIZE=10\n",
    "\n",
    "model=NN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "loss_function= nn.CrossEntropyLoss()\n",
    "optimizer= optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T04:35:13.056204Z",
     "start_time": "2018-07-23T04:35:13.052609Z"
    }
   },
   "outputs": [],
   "source": [
    "writer= SummaryWriter(comment=\"-batch-norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T04:36:00.066929Z",
     "start_time": "2018-07-23T04:35:13.431013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5] [000/937] mean_loss : 2.276\n",
      "[0/5] [500/937] mean_loss : 0.264\n",
      "[1/5] [000/937] mean_loss : 0.099\n",
      "[1/5] [500/937] mean_loss : 0.084\n",
      "[2/5] [000/937] mean_loss : 0.081\n",
      "[2/5] [500/937] mean_loss : 0.056\n",
      "[3/5] [000/937] mean_loss : 0.015\n",
      "[3/5] [500/937] mean_loss : 0.038\n",
      "[4/5] [000/937] mean_loss : 0.023\n",
      "[4/5] [500/937] mean_loss : 0.030\n",
      "CPU times: user 1min 6s, sys: 4.05 s, total: 1min 10s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        pred= model(inputs.view(len(inputs),-1))\n",
    "        loss= loss_function(pred, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.data.item())\n",
    "        if i % 500==0:\n",
    "            avg_loss= np.mean(losses)\n",
    "            print(\"[%d/%d] [%03d/%d] mean_loss : %.3f\" % (epoch,EPOCH,i,len(train_loader),avg_loss))\n",
    "            writer.add_scalars('data/batch_norm/',{'batch_norm': avg_loss}, (i+1)+(epoch*len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T04:36:00.109861Z",
     "start_time": "2018-07-23T04:36:00.105304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6006'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-23T04:56:30.717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/a408109/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "TensorBoard 1.9.0 at http://408109-HC-D16A57:6006 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir runs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T15:28:59.752327Z",
     "start_time": "2018-07-22T15:28:59.397023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112a0f170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as vdatasets\n",
    "import torchvision.utils as vutils\n",
    "from tensorboardX import SummaryWriter\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Drop out?\n",
    "- 딥러닝에서 사용하는 Regularization 기법 중 하나임.\n",
    "- 학습을 할 때 forward pass시 랜덤하게 일정 확률 만큼의 뉴런 출력값을 0으로 만듦.\n",
    "- 네트워크가 제한된 representation을 가지고도 작동할 수 있게함.\n",
    "- Drop out을 사용하는 것이 여러 sub network들을 ensemble한 결과로 볼 수 있음\n",
    "\n",
    "<img src = './images/dropout.png' width= 300>\n",
    "\n",
    "\n",
    "- 학습을 할 때 랜덤하게 일정 뉴런의 출력값을 0으로 만들어 실제 테스트 시에는 이러한 randomness를 평균해주기 위해 drop확률 만큼을 출력값에 곱해준다.  \n",
    "$ E[a]=$  \n",
    "$=w_{1}x+w_{2}y = \\frac{1}{4}(w_{1}x+w_{2}y) + \\frac{1}{4}(w_{1}x+0*y)+\\frac{1}{4}(0*x+w_{2}y)+\\frac{1}{4}(0*x+0*y)$  \n",
    "$=\\frac{1}{2}(w_{1}x+w_{2}y)$\n",
    "- 네트워크의 일부분만 학습을 해 학습속도가 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MNIST데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T15:29:00.512381Z",
     "start_time": "2018-07-22T15:29:00.459732Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE= 64\n",
    "train_dataset = vdatasets.MNIST(root='../data/MNIST/',\n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=False)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_dataset = vdatasets.MNIST(root='../data/MNIST/',\n",
    "                               train=False, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=False,\n",
    "                                           num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델\n",
    "\n",
    "Training vs Evaluation  \n",
    "\n",
    "Before training the model, it is imperative to call model.train(). Likewise, you must call model.eval() before testing the model. This corrects for the differences in dropout, batch normalization during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T15:29:01.330756Z",
     "start_time": "2018-07-22T15:29:01.316990Z"
    }
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0.5):\n",
    "        super(NN, self).__init__()\n",
    "        self.linear1= nn.Linear(input_size,hidden_size)\n",
    "        self.linear2= nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3= nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout= nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs= F.relu(self.linear1(inputs))\n",
    "        outputs= self.dropout(outputs)\n",
    "        outputs= F.relu(self.linear2(outputs))\n",
    "        outputs= self.dropout(outputs)\n",
    "        return self.linear3(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T15:29:01.826622Z",
     "start_time": "2018-07-22T15:29:01.812067Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (linear1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (linear3): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_SIZE=train_dataset[0][0].size()[1]**2\n",
    "HIDDEN_SIZE= 512\n",
    "OUTPUT_SIZE=10\n",
    "\n",
    "model= NN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "model.train() # train process\n",
    "\n",
    "model.eval() # test process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T15:29:03.218817Z",
     "start_time": "2018-07-22T15:29:03.203762Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCH=15\n",
    "LR=0.1\n",
    "INPUT_SIZE=train_dataset[0][0].size()[1]**2\n",
    "HIDDEN_SIZE= 512\n",
    "OUTPUT_SIZE=10\n",
    "BATCH_SIZE=64\n",
    "NUM_LAYERS=1\n",
    "DROPOUT=0.5\n",
    "\n",
    "model= NN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "loss_function= nn.CrossEntropyLoss()\n",
    "optimizer= optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T15:32:45.580694Z",
     "start_time": "2018-07-22T15:31:25.574754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/15] [000/938] mean_loss : 2.312\n",
      "[0/15] [500/938] mean_loss : 0.759\n",
      "[1/15] [000/938] mean_loss : 0.220\n",
      "[1/15] [500/938] mean_loss : 0.242\n",
      "[2/15] [000/938] mean_loss : 0.242\n",
      "[2/15] [500/938] mean_loss : 0.173\n",
      "[3/15] [000/938] mean_loss : 0.098\n",
      "[3/15] [500/938] mean_loss : 0.142\n",
      "[4/15] [000/938] mean_loss : 0.061\n",
      "[4/15] [500/938] mean_loss : 0.126\n",
      "[5/15] [000/938] mean_loss : 0.269\n",
      "[5/15] [500/938] mean_loss : 0.108\n",
      "[6/15] [000/938] mean_loss : 0.271\n",
      "[6/15] [500/938] mean_loss : 0.095\n",
      "[7/15] [000/938] mean_loss : 0.191\n",
      "[7/15] [500/938] mean_loss : 0.085\n",
      "[8/15] [000/938] mean_loss : 0.058\n",
      "[8/15] [500/938] mean_loss : 0.077\n",
      "[9/15] [000/938] mean_loss : 0.036\n",
      "[9/15] [500/938] mean_loss : 0.078\n",
      "[10/15] [000/938] mean_loss : 0.161\n",
      "[10/15] [500/938] mean_loss : 0.071\n",
      "[11/15] [000/938] mean_loss : 0.068\n",
      "[11/15] [500/938] mean_loss : 0.066\n",
      "[12/15] [000/938] mean_loss : 0.080\n",
      "[12/15] [500/938] mean_loss : 0.062\n",
      "[13/15] [000/938] mean_loss : 0.090\n",
      "[13/15] [500/938] mean_loss : 0.058\n",
      "[14/15] [000/938] mean_loss : 0.009\n",
      "[14/15] [500/938] mean_loss : 0.052\n",
      "CPU times: user 4min 56s, sys: 14.7 s, total: 5min 11s\n",
      "Wall time: 4min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        pred= model(inputs.view(len(inputs),-1))\n",
    "        loss= loss_function(pred, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.data.tolist())\n",
    "        if i % 500 == 0:\n",
    "            print(\"[%d/%d] [%03d/%d] mean_loss : %.3f\" % (epoch,EPOCH,i,len(train_loader),np.mean(losses)))\n",
    "            losses=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T15:39:19.137369Z",
     "start_time": "2018-07-22T15:39:19.127394Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation(data_loader, model):\n",
    "    model.eval() # for dropout at test time!\n",
    "    loss_function= nn.CrossEntropyLoss(size_average=False)\n",
    "    num_equal=0\n",
    "    losses=0\n",
    "    for inputs, targets in data_loader:\n",
    "        pred= model(inputs.view(len(inputs),-1))\n",
    "        losses += loss_function(pred, targets).data.tolist()\n",
    "        outputs= pred.max(1)[1] # argmax\n",
    "        num_equal += torch.eq(outputs, targets).sum().tolist()\n",
    "        \n",
    "    return num_equal/ len(data_loader.dataset), losses/len(data_loader.dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T15:42:12.716814Z",
     "start_time": "2018-07-22T15:42:09.511588Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyundai/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<train acc : > 0.994 / <train loss> : 0.019\n",
      "<test  acc : > 0.983 / <test  loss> : 0.057\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_loss= evaluation(train_loader, model)\n",
    "test_acc, test_loss= evaluation(test_loader, model)\n",
    "\n",
    "print('<train acc : > {} / <train loss> : {}'.format(round(train_acc,3), round(train_loss,3)))\n",
    "print('<test  acc : > {} / <test  loss> : {}'.format(round(test_acc,3), round(test_loss,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고\n",
    "- https://pytorch.org/docs/0.4.0/nn.html\n",
    "- https://pytorch.org/tutorials/\n",
    "- https://ratsgo.github.io/machine%20learning/2017/10/12/terms/\n",
    "- https://github.com/DSKSD/Pytorch_Fast_Campus_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T06:51:15.980029Z",
     "start_time": "2018-08-21T06:51:15.149701Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a PyTorch?\n",
    "- Tensor와 Optimizer, Neural Net등 GPU연산에 최적화된 모듈을 이용하여 빠르게 딥러닝 모델을 구현할 수 있는 프레임워크\n",
    "Facebook이 밀고 있던 lua기반의 torch를 python버전으로 포팅함.\n",
    "\n",
    "# > Pytorch Basic\n",
    "- Tensor\n",
    "- autograd\n",
    "- nn\n",
    "- optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. autograd\n",
    "- 딥러닝 프레임워크의 가장 큰 장점은 자동으로 미분을 계산해주는 것임.\n",
    "- 딥러닝에서 forward 단계에서 autograd은 수행하는 모든 연산을 기억함. 그리고 역전파 단계에서  미분을 계산한.\n",
    "\n",
    "### 2.1 Tensor\n",
    "- autograd에서 requires_grad=True로 설정되면 Tensor의 연산은 기록됨. 역전파(backward)연산 후에는 이 변수에 대한 gradient(변화도)는 .grad에 누적됨.\n",
    "- autorgrad구현에서 Function클래스도 매우 중요한데 Tensor와 Function은 상호 연결되어 있으며 모든 연산 과정을 부호화(encode)하여 순환하지 않는 그래프(acyclic graph)를 생성함. 각 변수는 .grad_fn속성을 갖는데 이는 생성한 Function을 참조하고 있음.(단, 사용자가 만든 Tensor는 예외로, 이 때 grad_fn은 None)임.\n",
    "- gradient를 계산하기 위해서는, Tensor의 .backward()를 호출하면 됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T06:51:17.545198Z",
     "start_time": "2018-08-21T06:51:17.537301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# tensor를 생성하고 연산을 추적하기 위해 requries_grad=True로 설정함.\n",
    "x= torch.ones(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T06:51:18.829276Z",
     "start_time": "2018-08-21T06:51:18.825263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T06:51:26.404379Z",
     "start_time": "2018-08-21T06:51:26.401358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad) # 현재 gradient를 계산하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T06:51:30.973174Z",
     "start_time": "2018-08-21T06:51:30.970340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn) # 생성된 텐서임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T06:51:44.271505Z",
     "start_time": "2018-08-21T06:51:44.267217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>)\n"
     ]
    }
   ],
   "source": [
    "# x에 대해 연산을 수행.\n",
    "y= x+2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T06:51:52.046482Z",
     "start_time": "2018-08-21T06:51:52.043702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward object at 0x116b524e0>\n"
     ]
    }
   ],
   "source": [
    "# 연산의 결과로 생성된 것이므로 grad_fn을 가짐.\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T06:51:59.928219Z",
     "start_time": "2018-08-21T06:51:59.922727Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward>)\n",
      "tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# y에 다른 연산을 수행\n",
    "z= y*y*3\n",
    "out= z.mean()\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 변화도\n",
    "- backwar에서 Tensor가 Scalar인 경우는 backward에 인자를 정해줄 필요가 없음.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T06:52:22.819074Z",
     "start_time": "2018-08-21T06:52:22.813857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward1>)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$out= \\frac{1}{4}\\sum_{i}z_{i}$$  \n",
    "$$z_{i}= 3(x_{i}+2)^{2}$$  \n",
    "$$\\frac{\\partial{out}}{\\partial{z_{i}}}=\\frac{z_{i}}{4}$$  \n",
    "$$\\frac{\\partial{z_{i}}}{\\partial{x_{i}}}=6(x_{i}+2)$$  \n",
    "$$\\frac{\\partial{out}}{\\partial{x_{i}}}=\\frac{\\partial{out}}{\\partial{z_{i}}} * \\frac{\\partial{z_{i}}}{\\partial{x_{i}}} = \\frac{z_{i}}{4}*6(x_{i}+2) =\\frac{3}{2}(x_{i}+2)$$  \n",
    "$$\\frac{\\partial{out}}{\\partial{x_{i}}}\\mid_{x_{i}=1}= \\frac{9}{2}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 여러개의 요소를 가질 때는 Tensor의 모양을 gradient의 인자로 지정해줄 필요가 있음.\n",
    "- 인자로 들어간 Tensor가 값이 1일 때는 순수한 gradient이고 값이 k일 때는 gradient*k가 계산됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T06:55:12.716425Z",
     "start_time": "2018-08-21T06:55:12.711866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad can be implicitly created only for scalar outputs\n"
     ]
    }
   ],
   "source": [
    "x= torch.ones(2,2, requires_grad=True)\n",
    "y= 2*x+1\n",
    "try:\n",
    "    y.backward()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T09:49:00.518442Z",
     "start_time": "2018-08-11T09:49:00.511349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x= torch.ones(2,2, requires_grad=True)\n",
    "y= 2*x+1\n",
    "try:\n",
    "    y.backward(torch.ones(2,2))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T09:49:00.758505Z",
     "start_time": "2018-08-11T09:49:00.751924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "x= torch.ones(2,2, requires_grad=True)\n",
    "y= 2*x+1\n",
    "try:\n",
    "    y.backward(torch.ones(2,2)*2)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T09:28:55.619524Z",
     "start_time": "2018-08-11T09:28:55.615555Z"
    }
   },
   "source": [
    "- 기본적으로 변화도 연산은 그래프 상의 모든 내부 버퍼를 새로쓰기(flush) 떄문에, 그래프의 특정 부분에 대해서 역전판 연산을 2번하고 싶다면, 첫 연산 단계에서 retain_variables=True의 값을 넘겨줘야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T09:49:01.506341Z",
     "start_time": "2018-08-11T09:49:01.495302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13.2908, 10.1354],\n",
      "        [ 8.4934, 12.9705]])\n",
      "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n"
     ]
    }
   ],
   "source": [
    "x= torch.randn(2,2, requires_grad=True)\n",
    "y= 2*x+2\n",
    "\n",
    "z= y*y+2\n",
    "z= z.sum()\n",
    "try:\n",
    "    z.backward() # retain_graph=True로 하면 내부 버퍼가 사라지는 것을 막아줌.\n",
    "\n",
    "    print(x.grad)\n",
    "\n",
    "    z.backward()\n",
    "    print(x.grad)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T09:49:01.950596Z",
     "start_time": "2018-08-11T09:49:01.937170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.3848,  6.6710],\n",
      "        [-4.1821, 11.0535]])\n",
      "tensor([[ 8.7695, 13.3419],\n",
      "        [-8.3643, 22.1069]])\n"
     ]
    }
   ],
   "source": [
    "x= torch.randn(2,2, requires_grad=True)\n",
    "y= 2*x+2\n",
    "\n",
    "z= y*y+2\n",
    "z= z.sum()\n",
    "try:\n",
    "    z.backward(retain_graph=True) # retain_graph=True로 하면 내부 버퍼가 사라지는 것을 막아줌.\n",
    "\n",
    "    print(x.grad)\n",
    "\n",
    "    z.backward()\n",
    "    print(x.grad)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with torch.no_grad():로 코드 블럭을 사용하면 autograd가 requires_graph=True인 Tensor들의 연산 기록을 추적을 제외할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T10:12:24.574701Z",
     "start_time": "2018-08-11T10:12:24.570130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. nn Module\n",
    "- nn.Module는 모든 neural network의 기본 class임.\n",
    "- nn.Module에는 레이어와 출력을 반환하는 메서드 forward가 포함되어있음.\n",
    "- 미리 포함된 forward함수에 input을 인자로 forward를 계산하고 Parameters의 backward를 계산할 수 있음. 그리고 parameter.grad를 생성함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 nn.Module로 Neural Network생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T10:30:20.230358Z",
     "start_time": "2018-08-11T10:30:20.221698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN(\n",
      "  (linear): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__() # 부모클래스 초기화\n",
    "        self.linear= nn.Linear(2,2) # nn.Linear부분도 다 nn.Module의 자식클래스\n",
    "        self.sigmoid= nn.Sigmoid() \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs= self.linear(inputs)\n",
    "        outputs= self.sigmoid(outputs)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "testnn= NN()\n",
    "\n",
    "print(testnn)\n",
    "inputs=torch.FloatTensor([1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 생성한 NN에 파라미터 혹은 서브 모듈에 접근\n",
    "- module class의 parameters메서드는 순차적으로 parameter를 반환하는 generator를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T10:30:34.491095Z",
     "start_time": "2018-08-11T10:30:34.486089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1025, -0.0028],\n",
      "        [ 0.6181,  0.2200]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2633, -0.4271], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in testnn.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- naemd_parameters는 순차적으로 parameter name과 parameter를 반환하는 generator를 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T10:31:20.801860Z",
     "start_time": "2018-08-11T10:31:20.794957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ linear.weight ] Parameter containing:\n",
      "tensor([[ 0.1025, -0.0028],\n",
      "        [ 0.6181,  0.2200]], requires_grad=True)\n",
      "[ linear.bias ] Parameter containing:\n",
      "tensor([-0.2633, -0.4271], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in testnn.named_parameters():\n",
    "    print('[',name,']', param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T10:31:40.994675Z",
     "start_time": "2018-08-11T10:31:40.991515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Sigmoid()\n"
     ]
    }
   ],
   "source": [
    "for child in testnn.children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 forward\n",
    "- 순차적으로 neural network를 계산함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T10:33:51.752514Z",
     "start_time": "2018-08-11T10:33:51.746130Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7627,  1.3969],\n",
      "        [-0.3245,  0.2879],\n",
      "        [ 1.0579,  0.9621]])\n",
      "==================================================\n",
      "tensor([[0.4145, 0.3564],\n",
      "        [0.4262, 0.3626],\n",
      "        [0.4607, 0.6079]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "inputs= torch.randn(3,2) # nn.Module은 기본적으로 dim=0는 batch size로 봄.\n",
    "print(inputs)\n",
    "print(\"=\"*50)\n",
    "print(testnn(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 nn.Sequential()\n",
    "- 여러 nn.Module을 순차적으로 처리할 수 있는 클래스를 생성.\n",
    "- 여러 모듈을 간단하게 관리할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T10:36:06.099999Z",
     "start_time": "2018-08-11T10:36:06.094341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Sigmoid()\n",
      ")\n",
      "tensor([[0.5839, 0.2755],\n",
      "        [0.6041, 0.3626],\n",
      "        [0.3237, 0.5535]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "model= nn.Sequential(nn.Linear(2,2),\n",
    "                    nn.Sigmoid())\n",
    "print(model)\n",
    "print(model(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OrderedDict로 이름과 함께 관리할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T10:38:13.110285Z",
     "start_time": "2018-08-11T10:38:13.103342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ])) \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. optimizer\n",
    "### 4.1 Loss\n",
    "- 최적화를 하기 이전에 Loss를 정의해야함.\n",
    "- Loss는 output(결과값 or 예측값)과 target(실제값)가 얼마나 차이가 나는지 나타내는 측도(measure)로 목적에 맞게 loss function을 정의해야함.\n",
    "    - example1. 몸무게를 예측하는 경우 MSELoss를 계삼함\n",
    "    - examapl2. 꽃의 종류를 예측하는 경우 CrossEntorpy를 계산함\n",
    "    \n",
    "- torch.nn 패키지에 Loss를 계산해주는 다양한 loss function들이 있음.\n",
    "    - L1Loss\n",
    "    - MSELoss\n",
    "    - CrossEntorpyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 nn.MSELoss\n",
    "- input과 target에 대한 Mean Sqaured Error를 계산함.\n",
    "$$ Loss(x,y) = \\frac{1}{n}\\sum_{i}(pred_{i}-target_{i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T11:03:38.501166Z",
     "start_time": "2018-08-11T11:03:38.495396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    }
   ],
   "source": [
    "loss_function= nn.MSELoss() \n",
    "preds= torch.FloatTensor([[1,2],[2,0]])\n",
    "targets= torch.ones(2,2)\n",
    "loss= loss_function(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 CrossEntropyLoss\n",
    "- Classification 문제에서 주로 사용되는 loss function으로 두 확률분포 사이의 차이를 재는 측도 중 하나임.\n",
    "- input과 target에 대한 CrossEntropyLoss를 계산함.\n",
    "- input은 FloatTensor type으로 클래스 수만큼 길이인 array로 구성됨.\n",
    "- target은 LongTensor로 정답인 클래스의 index로 구성됨.\n",
    "- CrossEntropy:\n",
    "$$ H(Target,Pred)= - \\sum_{x}Target(x)\\log(Q(x))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T11:20:10.575901Z",
     "start_time": "2018-08-11T11:20:10.570072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9037)\n"
     ]
    }
   ],
   "source": [
    "loss_function= nn.CrossEntropyLoss()\n",
    "preds= torch.FloatTensor([[0.7, 1.2, 0.6], [0.3, 1, 0.8]])\n",
    "targets= torch.LongTensor([1, 2])\n",
    "loss= loss_function(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 optim\n",
    "- torch.optim 패키지를 이용해 모델의 output(결과값)과 target(실제값)에 차이인 loss를 줄이기 위해 parameter update를 통해 최적화함.  \n",
    "- 'Gradient Descent'라는 방법을 사용해 neural network의 parameter update를 함.  \n",
    "$ param : \\theta$ , $ learning rate: \\alpha$ 라고 하면 다음과 같이 업데이트 함.$$ \\theta = \\theta - \\alpha * \\frac{\\partial{loss}}{\\partial{\\theta}}$$\n",
    "- torch.optim은 다양한 optimizer를 가짐.\n",
    "    - SGD\n",
    "    - RMSprop\n",
    "    - Adam\n",
    "    \n",
    "- 참고 사이트  \n",
    "    http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T11:38:46.495316Z",
     "start_time": "2018-08-11T11:38:46.483964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preds] tensor([[-0.8573],\n",
      "        [ 0.5466],\n",
      "        [ 1.3004],\n",
      "        [ 0.1584],\n",
      "        [ 0.1252]], grad_fn=<ThAddmmBackward>)\n",
      "[loss] tensor(1.0957, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "inputs= torch.randn(5,3)\n",
    "targets= torch.randn(5).unsqueeze(1)\n",
    "\n",
    "# model, loss function, optimizer 인스턴스를 생성\n",
    "model= nn.Linear(3,1)\n",
    "loss_function= nn.MSELoss()\n",
    "optimizer= optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "preds= model(inputs)\n",
    "print('[preds]', preds)\n",
    "loss=loss_function(preds, targets)\n",
    "print('[loss]',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T11:38:47.539257Z",
     "start_time": "2018-08-11T11:38:47.530426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward 전\n",
      "None\n",
      "None\n",
      "backward 후\n",
      "tensor([[-1.1525, -0.3281, -1.5363]])\n",
      "tensor([0.1765])\n"
     ]
    }
   ],
   "source": [
    "print('backward 전')\n",
    "for p in model.parameters():\n",
    "    print(p.grad)\n",
    "\n",
    "# 각 parameter에 대한 loss에 미분값을 계산\n",
    "loss.backward()\n",
    "\n",
    "print('backward 후')\n",
    "for p in model.parameters():\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전과 후를 비교해보면 각 parameter - grad * learning rate로 업데이트 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T11:38:48.404034Z",
     "start_time": "2018-08-11T11:38:48.394404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer 전\n",
      "tensor([[-0.1783, -0.5572, -0.4166]])\n",
      "tensor([0.2839])\n",
      "optimizer 전\n",
      "tensor([[-0.0631, -0.5244, -0.2630]])\n",
      "tensor([0.2663])\n"
     ]
    }
   ],
   "source": [
    "print('optimizer 전')\n",
    "for p in model.parameters():\n",
    "    print(p.data)\n",
    "\n",
    "# optimizer로 최적화함\n",
    "optimizer.step()\n",
    "\n",
    "print('optimizer 전')\n",
    "for p in model.parameters():\n",
    "    print(p.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
